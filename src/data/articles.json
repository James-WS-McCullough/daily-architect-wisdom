{
  "inspiration": [
    {
      "title": "Act with Prudence",
      "author": "Seb Rose",
      "content": "> Whatever you undertake, act with prudence and consider the consequences.\n~Anon\n\nNo matter how comfortable a schedule looks at the beginning of an iteration, you can’t avoid being under pressure some of the time. If you find yourself having to choose between “doing it right” and “doing it quick,” it is often appealing to “do it quick” with the understanding that you’ll come back and fix it later. When you make this promise to yourself, your team, and your customer, you mean it. But all too often, the next iteration brings new problems and you become focused on them. This sort of deferred work is known as technical debt, and it is not your friend. Specifically, Martin Fowler calls this deliberate technical debt in his taxonomy of technical debt,* and it should not be confused with inadvertent technical debt.\n\nTechnical debt is like a loan: you benefit from it in the short term, but you have to pay interest on it until it is fully paid off. Shortcuts in the code make it harder to add features or refactor your code. They are breeding grounds for defects and brittle test cases. The longer you leave it, the worse it gets. By the time you get around to undertaking the original fix, there may be a whole stack of not-quite-right design choices layered on top of the original problem, making the code much harder to refactor and correct. In fact, it is often only when things have got so bad that you must fix the original problem, that you actually do go back to fix it. And by then, it is often so hard to fix that you really can’t afford the time or the risk.\n\nThere are times when you must incur technical debt to meet a deadline or implement a thin slice of a feature. Try not to be in this position, but if the situation absolutely demands it, then go ahead. But (and this is a big but) you must track technical debt and pay it back quickly, or things go rapidly downhill.\n\nAs soon as you make the decision to compromise, write a task card or log it in your issue-tracking system to ensure that it does not get forgotten.\n\nIf you schedule repayment of the debt in the next iteration, the cost will be minimal. Leaving the debt unpaid will accrue interest, and that interest should be tracked to make the cost visible. This will emphasize the effect on business value of the project’s technical debt and enables appropriate prioritization of the repayment. The choice of how to calculate and track the interest will depend on the particular project, but track it you must.\n\nPay off technical debt as soon as possible. It would be imprudent to do otherwise."
    },
    {
      "title": "Apply Functional Programming Principles",
      "author": "Edward Garson",
      "content": "Functional programming has recently enjoyed renewed interest from the mainstream programming community. Part of the reason is because emergent properties of the functional paradigm are well positioned to address the challenges posed by our industry’s shift toward multicore. However, while that is certainly an important application, it is not the reason this piece admonishes you to know thy functional programming.\n\nMastery of the functional programming paradigm can greatly improve the quality of the code you write in other contexts. If you deeply understand and apply the functional paradigm, your designs will exhibit a much higher degree of referential transparency.\n\nReferential transparency is a very desirable property: it implies that functions consistently yield the same results given the same input, irrespective of where and when they are invoked. That is, function evaluation depends less—ideally, not at all—on the side effects of mutable state.\n\nA leading cause of defects in imperative code is attributable to mutable variables. Everyone reading this will have investigated why some value is not as expected in a particular situation. Visibility semantics can help to mitigate these insidious defects, or at least to drastically narrow down their location, but their true culprit may in fact be the providence of designs that employ inordinate mutability.\n\nAnd we certainly don’t get much help from the industry in this regard. Introductions to object orientation tacitly promote such design, because they often show examples composed of graphs of relatively long-lived objects that happily call mutator methods on one another, which can be ­ dangerous.\n\nHowever, with astute test-driven design, particularly when being sure to “Mock Roles, not Objects,” unnecessary mutability can be designed away.\n\nThe net result is a design that typically has better responsibility allocation with more numerous, smaller functions that act on arguments passed into them, rather than referencing mutable member variables. There will be fewer defects, and furthermore they will often be simpler to debug, because it is easier to locate where a rogue value is introduced in these designs than to otherwise deduce the particular context that results in an erroneous assignment. This adds up to a much higher degree of referential transparency, and positively nothing will get these ideas as deeply into your bones as learning a functional programming language, where this model of computation is the norm.\n\nOf course, this approach is not optimal in all situations. For example, in objectoriented systems, this style often yields better results with domain model development (i.e., where collaborations serve to break down the complexity of business rules) than with user-interface development.\n\nMaster the functional programming paradigm so you are able to judiciously apply the lessons learned to other domains. Your object systems (for one) will resonate with referential transparency goodness and be much closer to their functional counterparts than many would have you believe. In fact, some would even assert that, at their apex, functional programming and object orientation are merely a reflection of each other, a form of computational yin and yang."
    },
    {
      "title": "Ask, “What Would the User Do?” (You Are Not the User)",
      "author": "Giles Colborne",
      "content": "We all tend to assume that other people think like us. But they don’t. Psychologists call this the false consensus bias. When people think or act differently from us, we’re quite likely to label them (subconsciously) as defective in some way.\n\nThis bias explains why programmers have such a hard time putting themselves in the users’ position. Users don’t think like programmers. For a start, they spend much less time using computers. They neither know nor care how a computer works. This means they can’t draw on any of the battery of problem-solving techniques so familiar to programmers. They don’t recognize the patterns and cues programmers use to work with, through, and around an interface.\n\nThe best way to find out how a user thinks is to watch one. Ask a user to complete a task using a similar piece of software to what you’re developing.\n\nMake sure the task is a real one: “Add up a column of numbers” is OK; “Calculate your expenses for the last month” is better. Avoid tasks that are too specific, such as “Can you select these spreadsheet cells and enter a SUM formula below?”—there’s a big clue in that question. Get the user to talk through his or her progress. Don’t interrupt. Don’t try to help. Keep asking yourself, “Why is he doing that?” and “Why is she not doing that?” The first thing you’ll notice is that users do a core of things similarly. They try to complete tasks in the same order—and they make the same mistakes in the same places. You should design around that core behavior. This is different from design meetings, where people tend to listen when someone says, “What if the user wants to…?” This leads to elaborate features and confusion over what users want. Watching users eliminates this confusion.\n\nYou’ll see users getting stuck. When you get stuck, you look around. When users get stuck, they narrow their focus. It becomes harder for them to see solutions elsewhere on the screen. It’s one reason why help text is a poor solution to poor user interface design. If you must have instructions or help text, make sure to locate it right next to your problem areas. A user’s narrow focus of attention is why tool tips are more useful than help menus.\n\nUsers tend to muddle through. They’ll find a way that works and stick with it, no matter how convoluted. It’s better to provide one really obvious way of doing things than two or three shortcuts.\n\nYou’ll also find that there’s a gap between what users say they want and what they actually do. That’s worrying, as the normal way of gathering user requirements is to ask them. It’s why the best way to capture requirements is to watch users. Spending an hour watching users is more informative than spending a day guessing what they want."
    },
    {
      "title": "Automate Your Coding Standard",
      "author": "Filip van Laenen",
      "content": "You’ve probably been there, too. At the beginning of a project, everybody has lots of good intentions—call them “new project’s resolutions.” Quite often, many of these resolutions are written down in documents. The ones about code end up in the project’s coding standard. During the kick-off meeting, the lead developer goes through the document and, in the best case, everybody agrees that they will try to follow them. Once the project gets underway, though, these good intentions are abandoned, one at a time. When the project is finally delivered, the code looks like a mess, and nobody seems to know how it came to be that way.\n\nWhen did things go wrong? Probably already at the kick-off meeting. Some of the project members didn’t pay attention. Others didn’t understand the point.\n\nWorse, some disagreed and were already planning their coding standard rebellion. Finally, some got the point and agreed, but when the pressure in the project got too high, they had to let something go. Well-formatted code doesn’t earn you points with a customer that wants more functionality. Furthermore, following a coding standard can be quite a boring task if it isn’t automated. Just try to indent a messy class by hand to find out for yourself.\n\nBut if it’s such a problem, why is it that we want a coding standard in the first place? One reason to format the code in a uniform way is so that nobody can “own” a piece of code just by formatting it in his or her private way. We may want to prevent developers from using certain antipatterns in order to avoid some common bugs. In all, a coding standard should make it easier to work in the project, and maintain development speed from the beginning to the end.\n\nIt follows, then, that everybody should agree on the coding standard, too—it does not help if one developer uses three spaces to indent code, and another uses four.\n\nThere exists a wealth of tools that can be used to produce code quality reports and to document and maintain the coding standard, but that isn’t the whole solution. It should be automated and enforced where possible. Here are a few examples:\n\n- Make sure code formatting is part of the build process, so that everybody runs it automatically every time they compile the code.\n\n- Use static code analysis tools to scan the code for unwanted antipatterns.\n\nIf any are found, break the build.\n\n- Learn to configure those tools so that you can scan for your own, projectspecific antipatterns.\n\n- Do not only measure test coverage, but automatically check the results, too. Again, break the build if test coverage is too low.\n\nTry to do this for everything that you consider important. You won’t be able to automate everything you really care about. As for the things that you can’t automatically flag or fix, consider them a set of guidelines supplementary to the coding standard that is automated, but accept that you and your colleagues may not follow them as diligently.\n\nFinally, the coding standard should be dynamic rather than static. As the project evolves, the needs of the project change, and what may have seemed smart in the beginning isn’t necessarily smart a few months later."
    },
    {
      "title": "Beauty Is in Simplicity",
      "author": "Jørn Ølmheim",
      "content": "There is one quote, from plato, that I think is particularly good for all software developers to know and keep close to their hearts: *beauty of style and harmony and grace and good rhythm depends on simplicity.*\n\nIn one sentence, this sums up the values that we as software developers should aspire to.\n\nThere are a number of things we strive for in our code:\n\n- Readability\n\n- Maintainability\n\n- Speed of development\n\n- The elusive quality of beauty Plato is telling us that the enabling factor for all of these qualities is simplicity.\n\nWhat is beautiful code? This is potentially a very subjective question. Perception of beauty depends heavily on individual background, just as much of our perception of anything depends on our background. People educated in the arts have a different perception of (or at least approach to) beauty than people educated in the sciences. Arts majors tend to approach beauty in software by comparing software to works of art, while science majors tend to talk about symmetry and the golden ratio, trying to reduce things to formulae.\n\nIn my experience, simplicity is the foundation of most of the arguments from both sides.\n\nThink about source code that you have studied. If you haven’t spent time studying other people’s code, stop reading this right now and find some open source code to study. Seriously! I mean it! Go search the Web for some code in your language of choice, written by some well-known, acknowledged expert.\n\nYou’re back? Good. Where were we? Ah, yes…I have found that code that resonates with me, and that I consider beautiful, has a number of properties in common. Chief among these is simplicity. I find that no matter how complex the total application or system is, the individual parts have to be kept simple: simple objects with a single responsibility containing similarly simple, focused methods with descriptive names. Some people think the idea of having short methods of 5–10 lines of code is extreme, and some languages make it very hard to do, but I think that such brevity is a desirable goal nonetheless.\n\nThe bottom line is that beautiful code is simple code. Each individual part is kept simple with simple responsibilities and simple relationships with the other parts of the system. This is the way we can keep our systems maintainable over time, with clean, simple, testable code, ensuring a high speed of development throughout the lifetime of the system.\n\nBeauty is born of and found in simplicity."
    },
    {
      "title": "Before You Refactor",
      "author": "Rajith Attapattu",
      "content": "At some point , every programmer will need to refactor existing code. But before you do so, please think about the following, as this could save you and others a great deal of time (and pain):\n\n- The best approach for restructuring starts by taking stock of the existing codebase and the tests written against that code. This will help you understand the strengths and weaknesses of the code as it currently stands, so you can ensure that you retain the strong points while avoiding the mistakes. We all think we can do better than the existing system…until we end up with something no better—or even worse—than the previous incarnation because we failed to learn from the existing system’s mistakes.\n\n- Avoid the temptation to rewrite everything. It is best to reuse as much code as possible. No matter how ugly the code is, it has already been tested, reviewed, etc. Throwing away the old code—especially if it was in production—means that you are throwing away months (or years) of tested, battle-hardened code that may have had certain workarounds and bug fixes you aren’t aware of. If you don’t take this into account, the new code you write may end up showing the same mysterious bugs that were fixed in the old code. This will waste a lot of time, effort, and knowledge gained over the years.\n\n- Many incremental changes are better than one massive change. Incremental changes allows you to gauge the impact on the system more easily through feedback, such as from tests. It is no fun to see a hundred test failures after you make a change. This can lead to frustration and pressure that can in turn result in bad decisions. A couple of test failures at a time is easier to deal with, leading to a more manageable approach.\n\n- - - - After each development iteration, it is important to ensure that the existing tests pass. Add new tests if the existing tests are not sufficient to cover the changes you made. Do not throw away the tests from the old code without due consideration. On the surface, some of these tests may not appear to be applicable to your new design, but it would be well worth the effort to dig deep down into the reasons why this particular test was added.\n\nPersonal preferences and ego shouldn’t get in the way. If something isn’t broken, why fix it? That the style or the structure of the code does not meet your personal preference is not a valid reason for restructuring.\n\nThinking you could do a better job than the previous programmer is not a valid reason, either.\n\nNew technology is an insufficient reason to refactor. One of the worst reasons to refactor is because the current code is way behind all the cool technology we have today, and we believe that a new language or framework can do things a lot more elegantly. Unless a cost-benefit analysis shows that a new language or framework will result in significant improvements in functionality, maintainability, or productivity, it is best to leave it as it is.\n\nRemember that humans make mistakes. Restructuring will not always guarantee that the new code will be better—or even as good as—the previous attempt. I have seen and been a part of several failed restructuring attempts. It wasn’t pretty, but it was human."
    },
    {
      "title": "Beware the Share",
      "author": "Udi Dahan",
      "content": "It was my first project at the company . I’d just finished my degree and was anxious to prove myself, staying late every day going through the existing code. As I worked through my first feature, I took extra care to put in place everything I had learned—commenting, logging, pulling out shared code into libraries where possible, the works. The code review that I had felt so ready for came as a rude awakening—reuse was frowned upon!\n\nHow could this be? Throughout college, reuse was held up as the epitome of quality software engineering. All the articles I had read, the textbooks, the seasoned software professionals who taught me—was it all wrong?\n\nIt turns out that I was missing something critical.\n\nContext.\n\nThe fact that two wildly different parts of the system performed some logic in the same way meant less than I thought. Up until I had pulled out those libraries of shared code, these parts were not dependent on each other. Each could evolve independently. Each could change its logic to suit the needs of the system’s changing business environment. Those four lines of similar code were accidental—a temporal anomaly, a coincidence. That is, until I came along.\n\nThe libraries of shared code I created tied the shoelaces of each foot to the other. Steps by one business domain could not be made without first synchronizing with the other. Maintenance costs in those independent functions used to be negligible, but the common library required an order of magnitude more testing.\n\nWhile I’d decreased the absolute number of lines of code in the system, I had increased the number of dependencies. The context of these dependencies is critical—had they been localized, the sharing may have been justified and had some positive value. When these dependencies aren’t held in check, their tendrils entangle the larger concerns of the system, even though the code itself looks just fine.\n\nThese mistakes are insidious in that, at their core, they sound like a good idea.\n\nWhen applied in the right context, these techniques are valuable. In the wrong context, they increase cost rather than value. When coming into an existing codebase with no knowledge of where the various parts will be used, I’m much more careful these days about what is shared.\n\nBeware the share. Check your context. Only then, proceed."
    },
    {
      "title": "The Boy Scout Rule",
      "author": "Robert C. Martin (Uncle Bob)",
      "content": "The boy scouts ha ve a rule: “always leave the campground cleaner than you found it.” If you find a mess on the ground, you clean it up regardless of who might have made it. You intentionally improve the environment for the next group of campers. (Actually, the original form of that rule, written by Robert Stephenson Smyth Baden-Powell, the father of scouting, was “Try and leave this world a little better than you found it.”) What if we followed a similar rule in our code: “Always check a module in cleaner than when you checked it out”? Regardless of who the original author was, what if we always made some effort, no matter how small, to improve the module? What would be the result?\n\nI think if we all followed that simple rule, we would see the end of the relentless deterioration of our software systems. Instead, our systems would gradually get better and better as they evolved. We would also see teams caring for the system as a whole, rather than just individuals caring for their own small part.\n\nI don’t think this rule is too much to ask. You don’t have to make every module perfect before you check it in. You simply have to make it a little bit better than when you checked it out. Of course, this means that any code you add to a module must be clean. It also means that you clean up at least one other thing before you check the module back in. You might simply improve the name of one variable, or split one long function into two smaller functions.\n\nYou might break a circular dependency, or add an interface to decouple policy from detail.\n\nFrankly, this just sounds like common decency to me—like washing your hands after you use the restroom, or putting your trash in the bin instead of dropping it on the floor. Indeed, the act of leaving a mess in the code should be as socially unacceptable as littering. It should be something that just isn’t done.\n\nBut it’s more than that. Caring for our own code is one thing. Caring for the team’s code is quite another. Teams help one another and clean up after one another. They follow the Boy Scout rule because it’s good for everyone, not just good for themselves."
    },
    {
      "title": "Check Your Code First Before Looking to Blame Others",
      "author": "Allan Kelly",
      "content": "Developers—all of us!—often have trouble believing our own code is broken. It is just so improbable that, for once, it must be the compiler that’s broken.\n\nYet, in truth, it is very (very) unusual that code is broken by a bug in the compiler, interpreter, OS, app server, database, memory manager, or any other piece of system software. Yes, these bugs exist, but they are far less common than we might like to believe.\n\nI once had a genuine problem with a compiler bug optimizing away a loop variable, but I have imagined my compiler or OS had a bug many more times. I have wasted a lot of my time, support time, and management time in the process, only to feel a little foolish each time it turned out to be my mistake after all.\n\nAssuming that the tools are widely used, mature, and employed in various technology stacks, there is little reason to doubt the quality. Of course, if the tool is an early release, or used by only a few people worldwide, or a piece of seldom downloaded, version 0.1, open source software, there may be good reason to suspect the software. (Equally, an alpha version of commercial software might be suspect.) Given how rare compiler bugs are, you are far better putting your time and energy into finding the error in your code than into proving that the compiler is wrong. All the usual debugging advice applies, so isolate the problem, stub out calls, and surround it with tests; check calling conventions, shared libraries, and version numbers; explain it to someone else; look out for stack corruption and variable type mismatches; and try the code on different machines and different build configurations, such as debug and release.\n\nQuestion your own assumptions and the assumptions of others. Tools from different vendors might have different assumptions built into them—so too might different tools from the same vendor.\n\nWhen someone else is reporting a problem you cannot duplicate, go and see what they are doing. They may be doing something you never thought of or are doing something in a different order.\n\nMy personal rule is that if I have a bug I can’t pin down, and I’m starting to think it’s the compiler, then it’s time to look for stack corruption. This is especially true if adding trace code makes the problem move around.\n\nMultithreaded problems are another source of bugs that turn hair gray and induce screaming at the machine. All the recommendations to favor simple code are multiplied when a system is multithreaded. Debugging and unit tests cannot be relied on to find such bugs with any consistency, so simplicity of design is paramount.\n\nSo, before you rush to blame the compiler, remember Sherlock Holmes’s advice, “Once you eliminate the impossible, whatever remains, no matter how improbable, must be the truth,” and opt for it over Dirk Gently’s, “Once you eliminate the improbable, whatever remains, no matter how impossible, must be the truth.”"
    },
    {
      "title": "Choose Your Tools with Care",
      "author": "Giovanni Asproni",
      "content": "Modern applications are very rarel y built from scratch. They are assembled using existing tools—components, libraries, and frameworks— for a number of good reasons:\n\n- Applications grow in size, complexity, and sophistication, while the time available to develop them grows shorter. It makes better use of developers’ time and intelligence if they can concentrate on writing more ­ business-domain code and less infrastructure code.\n\n- Widely used components and frameworks are likely to have fewer bugs than the ones developed in-house.\n\n- There is a lot of high-quality software available on the Web for free, which means lower development costs and greater likelihood of finding developers with the necessary interest and expertise.\n\n- Software production and maintenance is human-intensive work, so buying may be cheaper than building.\n\nHowever, choosing the right mix of tools for your application can be a tricky business requiring some thought. In fact, when making a choice, you should keep in mind a few things:\n\n- Different tools may rely on different assumptions about their context—e.g., surrounding infrastructure, control model, data model, communication protocols, etc.—which can lead to an architectural mismatch between the application and the tools. Such a mismatch leads to hacks and workarounds that will make the code more complex than necessary.\n\n- Different tools have different lifecycles, and upgrading one of them may become an extremely difficult and time-consuming task since the new functionality, design changes, or even bug fixes may cause incompatibilities with \n\nthe other tools. The greater the number of tools, the worse the problem can become.\n\n- Some tools require quite a bit of configuration, often by means of one or more XML files, which can grow out of control very quickly. The application may end up looking as if it was all written in XML plus a few odd lines of code in some programming language. The configurational complexity will make the application difficult to maintain and to extend.\n\n- Vendor lock-in occurs when code that depends heavily on specific vendor products ends up being constrained by them on several counts: maintainability, performances, ability to evolve, price, etc.\n\n- If you plan to use free software, you may discover that it’s not so free after all. You may need to buy commercial support, which is not necessarily going to be cheap.\n\n- Licensing terms matter, even for free software. For example, in some companies, it is not acceptable to use software licensed under the GNU license terms because of its viral nature—i.e., software developed with it must be distributed along with its source code.\n\nMy personal strategy to mitigate these problems is to start small by using only the tools that are absolutely necessary. Usually the initial focus is on removing the need to engage in low-level infrastructure programming (and problems), e.g., by using some middleware instead of using raw sockets for distributed applications. And then add more if needed. I also tend to isolate the external tools from my business domain objects by means of interfaces and layering, so that I can change the tool if I have to with a minimal amount of pain. A positive side effect of this approach is that I generally end up with a smaller application that uses fewer external tools than originally forecast."
    },
    {
      "title": "Code in the Language of the Domain",
      "author": "Dan North",
      "content": "Picture two codebases. In one, you come across: \n```\nif(portfolioIdsByTraderId.get(trader.getId())\n.containsKey(portfolio.getId())) {...}\n```\n You scratch your head, wondering what this code might be for. It seems to be getting an ID from a trader object; using that to get a map out of a, well, mapof-maps, apparently; and then seeing if another ID from a portfolio object exists in the inner map. You scratch your head some more. You look for the declaration of portfolioIdsByTraderId and discover this: \n```\nMap<int, Map<int, int>> portfolioIdsByTraderId;\n```\n Gradually, you realize it might have something to do with whether a trader has access to a particular portfolio. And of course you will find the same lookup fragment—or, more likely, a similar but subtly different code fragment— whenever something cares whether a trader has access to a particular portfolio.\n\nIn the other codebase, you come across this: \n```\nif(trader.canView(portfolio))\n{...}\n```\n No head scratching. You don’t need to know how a trader knows. Perhaps there is one of these maps-of-maps tucked away somewhere inside. But that’s the trader’s business, not yours.\n\nNow which of those codebases would you rather be working in?\n\nOnce upon a time, we only had very basic data structures: bits and bytes and characters (really just bytes, but we would pretend they were letters and punctuation). Decimals were a bit tricky because our base-10 numbers don’t work very well in binary, so we had several sizes of floating-point types. Then came arrays and strings (really just different arrays). Then we had stacks and queues and hashes and linked lists and skip lists and lots of other exciting data structures that don’t exist in the real world. “Computer science” was about spending \n\nlots of effort mapping the real world into our restrictive data structures. The real gurus could even remember how they had done it.\n\nThen we got user-defined types! OK, this isn’t news, but it does change the game somewhat. If your domain contains concepts like traders and portfolios, you can model them with types called, say, Trader and Portfolio. But, more importantly than this, you can model relationships between them using domain terms, too.\n\nIf you don’t code using domain terms, you are creating a tacit (read: secret) understanding that this int over here means the way to identify a trader, whereas that int over there means the way to identify a portfolio. (Best not to get them mixed up!) And if you represent a business concept (“Some traders are not allowed to view some portfolios—it’s illegal”) with an algorithmic snippet—say, an existence relationship in a map of keys—you aren’t doing the audit and compliance guys any favors.\n\nThe next programmer to come along might not be in on the secret, so why not make it explicit? Using a key as a lookup to another key that performs an existence check is not terribly obvious. How is someone supposed to intuit that’s where the business rules preventing conflict of interest are implemented?\n\nMaking domain concepts explicit in your code means other programmers can gather the intent of the code much more easily than by trying to retrofit an algorithm into what they understand about a domain. It also means that when the domain model evolves—which it will, as your understanding of the domain grows—you are in a good position to evolve the code. Coupled with good encapsulation, the chances are good that the rule will exist in only one place, and that you can change it without any of the dependent code being any the wiser.\n\nThe programmer who comes along a few months later to work on the code will thank you. The programmer who comes along a few months later might be you."
    },
    {
      "title": "Code Is Design",
      "author": "Ryan Brush",
      "content": "Imagine waking up tomorrow and learning that the construction industry has made the breakthrough of the century. Millions of cheap, incredibly fast robots can fabricate materials out of thin air, have a near-zero power cost, and can repair themselves. And it gets better: given an unambiguous blueprint for a construction project, the robots can build it without human intervention, all at negligible cost.\n\nOne can imagine the impact on the construction industry, but what would happen upstream? How would the behavior of architects and designers change if construction costs were negligible? Today, physical and computer models are built and rigorously tested before investing in construction. Would we bother if the construction was essentially free? If a design collapses, no big deal—just find out what went wrong and have our magical robots build another one.\n\nThere are further implications. With models obsolete, unfinished designs evolve by repeatedly building and improving upon an approximation of the end goal. A casual observer may have trouble distinguishing an unfinished design from a finished product.\n\nOur ability to predict timelines will fade away. Construction costs are more easily calculated than design costs—we know the approximate cost of installing a girder, and how many girders we need. As predictable tasks shrink toward zero, the less predictable design time starts to dominate. Results are produced more quickly, but reliable timelines slip away.\n\nOf course, the pressures of a competitive economy still apply. With construction costs eliminated, a company that can quickly complete a design gains an \n\nedge in the market. Getting design done fast becomes the central push of engineering firms. Inevitably, someone not deeply familiar with the design will see an unvalidated version, see the market advantage of releasing early, and say, “This looks good enough.” Some life-or-death projects will be more diligent, but in many cases, consumers learn to suffer through the incomplete design. Companies can always send out our magic robots to “patch” the broken buildings and vehicles they sell.\n\nAll of this points to a startlingly counterintuitive conclusion: our sole premise was a dramatic reduction in construction costs, with the result that quality got worse.\n\nIt shouldn’t surprise us that the preceding story has played out in software.\n\nIf we accept that code is design—a creative process rather than a mechanical one—the software crisis is explained. We now have a design crisis: the demand for quality, validated designs exceeds our capacity to create them. The pressure to use incomplete design is strong.\n\nFortunately, this model also offers clues to how we can get better. Physical simulations equate to automated testing; software design isn’t complete until it is validated with a brutal battery of tests. To make such tests more effective, we are finding ways to rein in the huge state space of large systems. Improved languages and design practices give us hope. Finally, there is one inescapable fact: great designs are produced by great designers dedicating themselves to the mastery of their craft. Code is no different."
    },
    {
      "title": "Code Layout Matters",
      "author": "Steve Freeman",
      "content": "An infeasible number of years ago, i worked on a cobol system where staff members weren’t allowed to change the indentation unless they already had a reason to change the code, because someone once broke something by letting a line slip into one of the special columns at the beginning of a line. This applied even if the layout was misleading, which it sometimes was, so we had to read the code very carefully because we couldn’t trust it. The policy must have cost a fortune in programmer drag.\n\nThere’s research suggesting that we all spend much more of our programming time navigating and reading code—finding *where* to make the change—than actually typing, so that’s what we want to optimize for. Here are three such optimizations: \n\n**Easy to scan**\n\nPeople are really good at visual pattern matching (a leftover trait from the time when we had to spot lions on the savannah), so I can help myself by making everything that isn’t directly relevant to the domain—all the “accidental complexity” that comes with most commercial languages— fade into the background by standardizing it. If code that behaves the same looks the same, then my perceptual system will help me pick out the differences. That’s why I also observe conventions about how to lay out the parts of a class within a compilation unit: constants, fields, public methods, private methods.\n\n**Expressive layout**\n\nWe’ve all learned to take the time to find the right names so that our code expresses as clearly as possible what it does, rather than just listing the steps—right? The code’s layout is part of this expressiveness, too. A first cut is to have the team agree on an automatic formatter for the basics, and then I might make adjustments by hand while I’m coding. Unless there’s active dissension, a team will quickly converge on a common “hand-finished” style. A formatter cannot understand my intentions (I should know, I once wrote one), and it’s more important to me that the line breaks and groupings reflect the intention of the code, not just the syntax of the language. (Kevin McGuire freed me from my bondage to automatic code formatters.)\n\n**Compact format**\n\nThe more I can get on a screen, the more I can see without breaking context by scrolling or switching files, which means I can keep less state in my head. Long procedure comments and lots of whitespace made sense for eight-character names and line printers, but now I live in an IDE that does syntax coloring and cross linking. Pixels are my limiting factor, so I want every one to contribute to my understanding of the code. I want the layout to help me understand the code, but no more than that.\n\nA nonprogrammer friend once remarked that code looks like poetry. I get that feeling from really good code—that everything in the text has a purpose, and that it’s there to help me understand the idea. Unfortunately, writing code doesn’t have the same romantic image as writing poetry."
    },
    {
      "title": "Code Reviews",
      "author": "Mattias Karlsson",
      "content": "You should do code reviews. Why? Because they *increase code quality* and *reduce defect rate*. But not necessarily for the reasons you might think.\n\nBecause they may previously have had some bad experiences with code reviews, many programmers tend to dislike them. I have seen organizations that require that all code pass a formal review before being deployed to production. Often, it is the architect or a lead developer doing this review, a practice that can be described as *architect reviews everything*. This is stated in the company’s software development process manual, so the programmers must comply.\n\nThere may be some organizations that need such a rigid and formal process, but most do not. In most organizations, such an approach is counterproductive.\n\nReviewees can feel like they are being judged by a parole board. Reviewers need both the time to read the code and the time to keep up to date with all the details of the system; they can rapidly become the bottleneck in this process, and the process soon degenerates.\n\nInstead of simply correcting mistakes in code, the purpose of code reviews should be to *share knowledge* and establish common coding guidelines. Sharing your code with other programmers enables collective code ownership. Let a random team member *walk through the code* with the rest of the team.\n\nInstead of looking for errors, you should review the code by trying to learn and understand it.\n\nBe gentle during code reviews. Ensure that comments are *constructive, not caustic*. Introduce different roles for the review meeting to avoid having organizational seniority among team members affect the code review. Examples of roles could include having one reviewer focus on documentation, another on exceptions, and a third to look at the functionality. This approach helps to spread the review burden across the team members.\n\nHave a regular *code review day* each week. Spend a couple of hours in a review meeting. Rotate the reviewee every meeting in a simple round-robin pattern. Remember to switch roles among team members every review meeting, too.\n\n*Involve newbies* in code reviews. They may be inexperienced, but their fresh university knowledge can provide a different perspective. *Involve experts* for their experience and knowledge. They will identify error-prone code faster and with more accuracy. Code reviews will flow more easily if the team has coding conventions that are checked by tools. That way, code formatting will never be discussed during the code review meeting.\n\n*Making code reviews* fun is perhaps the most important contributor to success. Reviews are about the people reviewing. If the review meeting is painful or dull, it will be hard to motivate anyone. Make it an *informal code review* whose principal purpose is to share knowledge among team members. Leave sarcastic comments outside, and bring a cake or brown-bag lunch instead."
    },
    {
      "title": "Coding with Reason",
      "author": "Yechiel Kimchi",
      "content": "Trying to reason about software correctness by hand results in a formal proof that is longer than the code, and more likely to contain errors. Automated tools are preferable but not always possible. What follows describes a middle path: reasoning semiformally about correctness.\n\nThe underlying approach is to divide all the code under consideration into short sections—from a single line, such as a function call, to blocks of less than 10 lines—and argue about their correctness. The arguments need only be strong enough to convince your devil’s advocate peer programmer.\n\nA section should be chosen so that at each endpoint, the state of the program (namely, the program counter and the values of all “living” objects) satisfies an easily described property, and so that the functionality of that section (state transformation) is easy to describe as a single task; these guidelines will make reasoning simpler. Such endpoint properties generalize concepts like preconditions and postconditions for functions, and invariants for loops and classes (with respect to their instances). Striving for sections to be as independent of one another as possible simplifies reasoning and is indispensable when these sections are to be modified.\n\nMany of the coding practices that are well known (although perhaps less well followed) and considered “good” make reasoning easier. Hence, just by intending to reason about your code, you already start moving toward a better style and structure. Unsurprisingly, most of these practices can be checked by static code analyzers:\n\n- Avoid using goto statements, as they make remote sections highly interdependent.\n\n- Avoid using modifiable global variables, as they make all sections that use them dependent.\n\n- Each variable should have the smallest possible scope. For example, a local object can be declared right before its first usage.\n\n- Make objects immutable whenever relevant.\n\n- Make the code readable by using spacing, both horizontal and vertical—e.g., aligning related structures and using an empty line to separate two sections.\n\n- Make the code self-documenting by choosing descriptive (but relatively short) names for objects, types, functions, etc.\n\n- If you need a nested section, make it a function.\n\n- Make your functions short and focused on a single task. The old 24-line limit still applies. Although screen size and resolution have changed, nothing has changed in human cognition since the 1960s.\n\n- Functions should have few parameters (four is a good upper bound). This does not restrict the data communicated to functions: grouping related parameters into a single object localizes object invariants, which simplifies reasoning with respect to their coherence and consistency.\n\n- More generally, each unit of code, from a block to a library, should have a narrow interface. Less communication reduces the reasoning required. This means that getters that return internal state are a liability—don’t ask an object for information to work with. Instead, ask the object to do the work with the information it already has. In other words, encapsulation is all—and only—about narrow interfaces.\n\n- In order to preserve class invariants, usage of setters should be discouraged. Setters tend to allow invariants that govern an object’s state to be broken.\n\nAs well as reasoning about its correctness, arguing about your code helps you better understand it. Communicate the insights you gain for everyone’s benefit."
    },
    {
      "title": "A Comment on Comments",
      "author": "Cal Evans",
      "content": "In my first programming class in college, my teacher handed out two basic coding sheets. On the board, the assignment read, “Write a program to input and average 10 bowling scores.” Then the teacher left the room.\n\nHow hard could this be? I don’t remember my final solution, but I’m sure it had a `FOR/NEXT` loop in it and couldn’t have been more than 15 lines long in total.\n\nCoding sheets—for you kids reading this, yes, we used to write code out longhand before actually entering it into a computer—allowed for around 70 lines of code each. I was very confused as to why the teacher would have given us two sheets. Since my handwriting has always been atrocious, I used the second one to recopy my code very neatly, hoping to get a couple of extra points for style.\n\nMuch to my surprise, when I received the assignment back at the start of the next class, I received a barely passing grade. (It was to be an omen to me for the rest of my time in college.) Scrawled across the top of my neatly copied code was “No comments?” It was not enough that the teacher and I both knew what the program was supposed to do. Part of the point of the assignment was to teach me that my code should explain itself to the next programmer coming behind me. It’s a lesson I’ve not forgotten.\n\nComments are not evil. They are as necessary to programming as basic branching or looping constructs. Most modern languages have a tool akin to javadoc that will parse properly formatted comments to automatically build an API document. This is a very good start, but not nearly enough. Inside your code should be explanations about what the code is supposed to be doing. Coding by the old adage, “If it was hard to write, it should be hard to read,” does a disservice to your client, your employer, your colleagues, and your future self.\n\nOn the other hand, you can go too far in your commenting. Make sure that your comments clarify your code but do not obscure it. Sprinkle your code with relevant comments explaining what the code is supposed to accomplish.\n\nYour header comments should give any programmer enough information to use your code without having to read it, while your inline comments should assist the next developer in fixing or extending it.\n\nAt one job, I disagreed with a design decision made by those above me. Feeling rather snarky, as young programmers often do, I pasted the text of the email instructing me to use their design into the header comment block of the file. It turned out that managers at this particular shop actually reviewed the code when it was committed. It was my first introduction to the term *career-limiting move*."
    },
    {
      "title": "Comment Only What the Code Cannot Say",
      "author": "Kevlin Henney",
      "content": "The difference between theory and practice is greater in practice than it is in theory—an observation that certainly applies to comments. In theory, the general idea of commenting code sounds like a worthy one: offer the reader detail, an explanation of what’s going on. What could be more helpful than being helpful? In practice, however, comments often become a blight.\n\nAs with any other form of writing, there is a skill to writing good comments.\n\nMuch of the skill is in knowing when not to write them.\n\nWhen code is ill-formed, compilers, interpreters, and other tools will be sure to object. If the code is in some way functionally incorrect, reviews, static analysis, tests, and day-to-day use in a production environment will flush most bugs out. But what about comments? In *The Elements of Programming Style* (Computing McGraw-Hill), Kernighan and Plauger note that “a comment is of zero (or negative) value if it is wrong.” And yet such comments often litter and survive in a codebase in a way that coding errors never could.\n\nThey provide a constant source of distraction and misinformation, a subtle but constant drag on a programmer’s thinking.\n\nWhat of comments that are not technically wrong, but add no value to the code? Such comments are noise. Comments that parrot the code offer nothing extra to the reader—stating something once in code and again in natural language does not make it any truer or more real. Commented-out code is not executable code, so it has no useful effect for either reader or runtime. It also becomes stale very quickly. Version-related comments and commented-out code try to address questions of versioning and history. These questions have already been answered (far more effectively) by version control tools.\n\nA prevalence of noisy comments and incorrect comments in a codebase encourages programmers to ignore all comments, either by skipping past them or by taking active measures to hide them. Programmers are resourceful and will route around anything perceived to be damage: folding comments up; switching coloring scheme so that comments and the background are the same color; scripting to filter out comments. To save a codebase from such misapplications of programmer ingenuity, and to reduce the risk of overlooking any comments of genuine value, comments should be treated as though they were code. Each comment should add some value for the reader, otherwise it is waste that should be removed or rewritten.\n\nWhat then qualifies as value? Comments should say something code does not and cannot say. A comment explaining what a piece of code should already say is an invitation to change code structure or coding conventions so the code speaks for itself. Instead of compensating for poor method or class names, rename them. Instead of commenting sections in long functions, extract smaller functions whose names capture the former sections’ intent. Try to express as much as possible through code. Any shortfall between what you can express in code and what you would like to express in total becomes a plausible candidate for a useful comment. Comment what the code *cannot* say, not simply what it does not say."
    },
    {
      "title": "Continuous Learning",
      "author": "Clint Shank",
      "content": "We live in interesting times. As development gets distributed across the globe, you learn there are lots of people capable of doing your job. You need to keep learning to stay marketable. Otherwise you’ll become a dinosaur, stuck in the same job until, one day, you’ll no longer be needed or your job gets outsourced to some cheaper resource.\n\nSo what do you do about it? Some employers are generous enough to provide training to broaden your skill set. Others may not be able to spare the time or money for any training at all. To play it safe, you need to take responsibility for your own education.\n\nHere’s a list of ways to keep you learning. Many of these can be found on the Internet for free:\n\n- Read books, magazines, blogs, Twitter feeds, and websites. If you want to go deeper into a subject, consider joining a mailing list or newsgroup. If you really want to get immersed in a technology, get hands on—write some code.\n\n- Always try to work with a mentor, as being the top guy can hinder your education. Although you can learn something from anybody, you can learn a whole lot more from someone smarter or more experienced than you. If you can’t find a mentor, consider moving on.\n\n- Use virtual mentors. Find authors and developers on the Web who you really like and read everything they write. Subscribe to their blogs.\n\n- Get to know the frameworks and libraries you use. Knowing how something works makes you know how to use it better. If they’re open source, you’re really in luck. Use the debugger to step through the code to see what’s going on under the hood. You’ll get to see code written and reviewed by some really smart people.\n\n- Whenever you make a mistake, fix a bug, or run into a problem, try to really understand what happened. It’s likely that someone else ran into the same problem and posted it on the Web. Google is really useful here.\n\n- A good way to learn something is to teach or speak about it. When people are going to listen to you and ask you questions, you’ll be highly motivated to learn. Try a lunch-’n’-learn at work, a user group, or a local conference.\n\n- Join or start a study group (à la patterns community) or a local user group for a language, technology, or discipline you are interested in.\n\n- Go to conferences. And if you can’t go, many conferences put their talks online for free.\n\n- Long commute? Listen to podcasts.\n\n- Ever run a static analysis tool over the codebase or look at the warnings in your IDE? Understand what they’re reporting and why.\n\n- Follow the advice of the *Pragmatic Programmers* and learn a new language every year. At least learn a new technology or tool. Branching out gives you new ideas you can use in your current technology stack.\n\n- Not everything you learn has to be about technology. Learn the domain you’re working in so you can better understand the requirements and help solve the business problem. Learning how to be more productive— how to work better—is another good option.\n\n- Go back to school.\n\nIt would be nice to have the capability that Neo had in The Matrix, and simply download the information we need into our brains. But we don’t, so it will take a time commitment. You don’t have to spend every waking hour learning. A little time—say, each week—is better than nothing. There is (or should be) a life outside of work.\n\nTechnology changes fast. Don’t get left behind."
    },
    {
      "title": "Convenience Is Not an -ility",
      "author": "Gregor Hohpe",
      "content": "Much has been said about the importance and challenges of designing good apis. It’s difficult to get right the first time and it’s even more difficult to change later—sort of like raising children. Most experienced programmers have learned that a good API follows a consistent level of abstraction, exhibits consistency and symmetry, and forms the vocabulary for an expressive language. Alas, being aware of the guiding principles does not automatically translate into appropriate behavior. Eating sweets is bad for you.\n\nInstead of preaching from on high, I want to pick on a particular API design “strategy,” one that I encounter time and again: the argument of convenience.\n\nIt typically begins with one of the following “insights”:\n\n- I don’t want other classes to have to make two separate calls to do this one thing.\n\n- Why should I make another method if it’s almost the same as this method?\n\nI’ll just add a simple switch.\n\n- See, it’s very easy: if the second string parameter ends with “.txt”, the method automatically assumes that the first parameter is a filename, so I really don’t need two methods.\n\nWhile well intended, such arguments are prone to decrease the readability of code using the API. A method invocation like:\n\n`parser.processNodes(text, false);`\n\nis virtually meaningless without knowing the implementation or at least consulting the documentation. This method was likely designed for the convenience of the implementer as opposed to the convenience of the caller—“I don’t want the caller to have to make two separate calls” translated into “I didn’t want to code up two separate methods.” There’s nothing fundamentally wrong with convenience if it’s intended to be the antidote to tediousness, clunkiness, or awkwardness. However, if we think a bit more carefully about it, the antidote to those symptoms is efficiency, consistency, and elegance, not necessarily convenience. APIs are supposed to hide underlying complexity, so we can realistically expect good API design to require some effort. A single large method could certainly be more convenient to write than a well-thought-out set of operations, but would it be easier to use?\n\nThe metaphor of API as a language can guide us toward better design decisions in these situations. An API should provide an expressive language, which gives the next layer above sufficient vocabulary to ask and answer useful questions.\n\nThis does not imply that it should provide exactly one method, or verb, for each question that may be worth asking. A diverse vocabulary allows us to express subtleties in meaning. For example, we prefer to say `run` instead of `walk(true)`, even though it could be viewed as essentially the same operation, just executed at different speeds. A consistent and well-thought-out API vocabulary makes for expressive and easy-to-understand code in the next layer up. More importantly, a composable vocabulary allows other programmers to use the API in ways you may not have anticipated—a great convenience indeed for the users of the API! Next time you are tempted to lump a few things together into one API method, remember that the English language does not have one word for `MakeUpYourRoomBeQuietAndDoYourHomeWork`, even though it would be really convenient for such a frequently requested operation."
    },
    {
      "title": "Deploy Early and Often",
      "author": "Steve Berczuk",
      "content": "Debugging the deployment and installation processes is often put off until close to the end of a project. In some projects, writing installation tools is delegated to a release engineer who takes on the task as a “necessary evil.” Reviews and demonstrations are done from a hand-crafted environment to ensure that everything works. The result is that the team gets no experience with the deployment process or the deployed environment until it may be too late to make changes.\n\nThe installation/deployment process is the first thing that the customer sees, and a simple one is the first step to having a reliable (or, at least, easy to debug) production environment. The deployed software is what the customer will use. By not ensuring that the deployment sets up the application correctly, you’ll raise questions with your customers before they get to use your software thoroughly.\n\nStarting your project with an installation process will give you time to evolve the process as you move through the product development cycle, and the chance to make changes to the application code to make the installation easier.\n\nRunning and testing the installation process on a clean environment periodically also provides a check that you have not made assumptions in the code that rely on the development or test environments.\n\nPutting deployment last means that the deployment process may need to be more complicated to work around assumptions in the code. What seemed a great idea in an IDE, where you have full control over an environment, might make for a much more complicated deployment process. It is better to know all the trade-offs sooner rather than later.\n\nWhile “being able to deploy” doesn’t seem to have a lot of business value early on as compared to seeing an application run on a developer’s laptop, the simple truth is that until you can demonstrate you application on the target environment, there is a lot of work to do before you can deliver business value. If your rationale for putting off a deployment process is that it is trivial, then do it anyway since it is low cost. If it’s too complicated, or if there are too many uncertainties, do what you would do with application code: experiment, evaluate, and refactor the deployment process as you go.\n\nThe installation/deployment process is essential to the productivity of your customers or your professional services team, so you should be testing and refactoring this process as you go. We test and refactor the source code throughout a project. The deployment deserves no less."
    },
    {
      "title": "Distinguish Business Exceptions from Technical",
      "author": "Dan Bergh Johnsson",
      "content": "There are basically two reasons that things go wrong at runtime: technical problems that prevent us from using the application and business logic that prevents us from misusing the application. Most modern languages, such as LISP, Java, Smalltalk, and C#, use exceptions to signal both these situations. However, the two situations are so different that they should be carefully held apart. It is a potential source of confusion to represent them both using the same exception hierarchy, not to mention the same exception class.\n\nAn unresolvable technical problem can occur when there is a programming error. For example, if you try to access element 83 from an array of size 17, then the program is clearly off track, and some exception should result. The subtler version is calling some library code with inappropriate arguments, causing the same situation on the inside of the library.\n\nIt would be a mistake to attempt to resolve these situations you caused yourself. Instead, we let the exception bubble up to the highest architectural level and let some general exception-handling mechanism do what it can to ensure that the system is in a safe state, such as rolling back a transaction, logging and alerting administration, and reporting back (politely) to the user.\n\nA variant of this situation is when you are in the “library situation” and a caller has broken the contract of your method, e.g., passing a totally bizarre argument or not having a dependent object set up properly. This is on a par with accessing the 83rd element from 17: the caller should have checked; not doing so is a programmer error on the client side. The proper response is to throw a technical exception.\n\nA different, but still technical, situation is when the program cannot proceed because of a problem in the execution environment, such as an unresponsive database. In this situation, you must assume that the infrastructure did what it could to resolve the issue—repairing connections and retrying a reasonable number of times—and failed. Even if the cause is different, the situation for the calling code is similar: there is little it can do about it. So, we signal the situation through an exception that we let bubble up to the general exception-handling mechanism.\n\nIn contrast to these, we have the situation where you cannot complete the call for a domain-logical reason. In this case, we have encountered a situation that is an exception, i.e., unusual and undesirable, but not bizarre or programmatically in error (for example, if I try to withdraw money from an account with insufficient funds). In other words, this kind of situation is a part of the contract, and throwing an exception is just an alternative return path that is part of the model and that the client should be aware of and be prepared to handle.\n\nFor these situations, it is appropriate to create a specific exception or a separate exception hierarchy so that the client can handle the situation on its own terms.\n\nMixing technical exceptions and business exceptions in the same hierarchy blurs the distinction and confuses the caller about what the method contract is, what conditions it is required to ensure before calling, and what situations it is supposed to handle. Separating the cases gives clarity and increases the chances that technical exceptions will be handled by some application framework, while the business domain exceptions actually are considered and handled by the client code."
    },
    {
      "title": "Do Lots of Deliberate Practice",
      "author": "Jon Jagger",
      "content": "Deliberate practice is not simply performing a task. If you ask yourself, “Why am I performing this task?” and your answer is, “To complete the task,” then you’re not doing deliberate practice.\n\nYou do deliberate practice to improve your ability to perform a task. It’s about skill and technique. Deliberate practice means repetition. It means performing the task with the aim of increasing your mastery of one or more aspects of the task. It means repeating the repetition. Slowly, over and over again, until you achieve your desired level of mastery. You do deliberate practice to master the task, not to complete the task.\n\nThe principal aim of paid development is to finish a product, whereas the principal aim of deliberate practice is to improve your performance. They are not the same. Ask yourself, how much of your time do you spend developing someone else’s product? How much developing yourself?\n\nHow much deliberate practice does it take to acquire expertise?\n\n- Peter Norvig writes* that “it may be that 10,000 hours…is the magic number.”\n\n- In *Leading Lean Software Development* (Addison-Wesley Professional), Mary Poppendieck notes that “it takes elite performers a minimum of 10,000 hours of deliberate focused practice to become experts.”\n\nThe expertise arrives gradually over time—not all at once in the 10,000th hour! Nevertheless, 10,000 hours is a lot: about 20 hours per week for 10 years.\n\nGiven this level of commitment, you might be worrying that you’re just not expert material. You are. Greatness is largely a matter of conscious choice.\n\nYour choice. Research over the last two decades has shown that the main factor in acquiring expertise is time spent doing deliberate practice. Innate ability is not the main factor. According to Mary Poppendieck:\n\n> There is broad consensus among researchers of expert performance that inborn talent does not account for much more than a threshold; you have to have a minimum amount of natural ability to get started in a sport or profession. After that, the people who excel are the ones who work the hardest.\n\nThere is little point to deliberately practicing something you are already an expert at. Deliberate practice means practicing something you are not good at. Peter Norvig explains: \n\n> The key [to developing expertise] is deliberative practice: not just doing it again and again, but challenging yourself with a task that is just beyond your current ability, trying it, analyzing your performance while and after doing it, and correcting any mistakes.\n\nAnd Mary Poppendieck writes: \n\n> Deliberate practice does not mean doing what you are good at; it means challenging yourself, doing what you are not good at. So it’s not necessarily fun.\n\nDeliberate practice is about learning—learning that changes you, learning that changes your behavior. Good luck."
    },
    {
      "title": "Domain-Specific Languages",
      "author": "Michael Hunger",
      "content": "Whenever you listen to a discussion by experts in any domain, be it chess players, kindergarten teachers, or insurance agents, you’ll notice that their vocabulary is quite different from everyday language. That’s part of what domain-specific languages (DSLs) are about: a specific domain has a specialized vocabulary to describe the things that are particular to that domain.\n\nIn the world of software, DSLs are about executable expressions in a language specific to a domain, employing a limited vocabulary and grammar that is readable, understandable, and—hopefully—writable by domain experts. DSLs targeted at software developers or scientists have been around for a long time.\n\nThe Unix “little languages” found in configuration files and the languages created with the power of LISP macros are some of the older examples.\n\nDSLs are commonly classified as either *internal* or *external*: \n\n**Internal DSLs** \nAre written in a general-purpose programming language whose syntax has been bent to look much more like natural language. This is easier for languages that offer more syntactic sugar and formatting possibilities (e.g., Ruby and Scala) than it is for others that do not (e.g., Java). Most internal DSLs wrap existing APIs, libraries, or business code and provide a wrapper for less mind-bending access to the functionality. They are directly executable by just running them. Depending on the implementation and the domain, they are used to build data structures, define dependencies, run processes or tasks, communicate with other systems, or validate user input. The syntax of an internal DSL is constrained by the host language. There are many patterns—e.g., expression builder, method chaining, and annotation—that can help you to bend the host language to your DSL. If the host language doesn’t require recompilation, an internal DSL can be developed quite quickly working side by side with a domain expert.\n\n**External DSLs**\nAre textual or graphical expressions of the language—although textual DSLs tend to be more common than graphical ones. Textual expressions can be processed by a toolchain that includes lexer, parser, model transformer, generators, and any other type of post-processing. External DSLs are mostly read into internal models that form the basis for further processing. It is helpful to define a grammar (e.g., in EBNF). A grammar provides the starting point for generating parts of the toolchain (e.g., editor, visualizer, parser generator).For simple DSLs, a handmade parser may be sufficient—using, for instance, regular expressions. Custom parsers can become unwieldy if too much is asked of them, so it makes sense to look at tools designed specifically for working with language grammars and DSLs—e.g., openArchitectureWare, ANTLR, SableCC, AndroMDA. Defining external DSLs as XML dialects is also quite common, although readability is often an issue—especially for nontechnical readers.\n\nYou must always take the target audience of your DSL into account. Are they developers, managers, business customers, or end users? You have to adapt the technical level of the language, the available tools, syntax help (e.g., IntelliSense), early validation, visualization, and representation to the intended audience. By hiding technical details, DSLs can empower users by giving them the ability to adapt systems to their needs without requiring the help of developers. It can also speed up development because of the potential distribution of work after the initial language framework is in place. The language can be evolved gradually. There are also different migration paths for existing expressions and grammars available."
    },
    {
      "title": "Don’t Be Afraid to Break Things",
      "author": "Mike Lewis",
      "content": "Everyone with industry experience has undoubtedly worked on a project where the codebase was precarious at best. The system is poorly factored, and changing one thing always manages to break another unrelated feature. Whenever a module is added, the coder’s goal is to change as little as possible, and hold his breath during every release. This is the software equivalent of playing *Jenga* with I-beams in a skyscraper, and is bound for disaster.\n\nThe reason that making changes is so nerve-racking is because the system is sick. It needs a doctor, otherwise its condition will only worsen. You already know what is wrong with your system, but you are afraid of breaking the eggs to make your omelet. A skilled surgeon knows that cuts have to be made in order to operate, but she also knows that the cuts are temporary and will heal.\n\nThe end result of the operation is worth the initial pain, and the patient should heal to a better state than he was in before the surgery.\n\nDon’t be afraid of your code. Who cares if something gets temporarily broken while you move things around? A paralyzing fear of change is what got your project into this state to begin with. Investing the time to refactor will pay for itself several times over the lifecycle of your project. An added benefit is that your team’s experience dealing with the sick system makes you all experts in knowing how it *should* work. Apply this knowledge rather than resent it. Working on a system you hate is not how anybody should have to spend his time.\n\nRedefine internal interfaces, restructure modules, refactor copy–pasted code, and simplify your design by reducing dependencies. You can significantly reduce code complexity by eliminating corner cases, which often result from improperly coupled features. Slowly transition the old structure into the new one, testing along the way. Trying to accomplish a large refactor in “one big shebang” will cause enough problems to make you consider abandoning the whole effort midway through.\n\nBe the surgeon who isn’t afraid to cut out the sick parts to make room for healing. The attitude is contagious and will inspire others to start working on those cleanup projects they’ve been putting off. Keep a “hygiene” list of tasks that the team feels are worthwhile for the general good of the project. Convince management that even though these tasks may not produce visible results, they will reduce expenses and expedite future releases. Never stop caring about the general “health” of the code."
    },
    {
      "title": "Don’t Be Cute with Your Test Data",
      "author": "Mike Lewis",
      "content": "> It was getting late. I was throwing in some placeholder data to test the page layout I’d been working on.\nI appropriated the members of The Clash for the names of users. Company names? Song titles by the Sex Pistols would do. Now I needed some stock ticker symbols—just some four-letter words in capital letters.\nI used **those** four-letter words.\nIt seemed harmless. Just something to amuse myself, and maybe the other developers the next day before I wired up the real data source.\nThe following morning, a project manager took some screenshots for a presentation.\n\nProgramming history is littered with these kinds of war stories. Things that developers and designers did “that no one else would see,” which unexpectedly became visible.\n\nThe leak type can vary but, when it happens, it can be deadly to the person, team, or company responsible. Examples include:\n\n- During a status meeting, a client clicks on a button that is as yet unimplemented. He is told, “Don’t click that again, you moron.”\n\n- A programmer maintaining a legacy system has been told to add an error dialog, and decides to use the output of existing behind-the-scenes logging to power it. Users are suddenly faced with messages such as “Holy database commit failure, Batman!” when something breaks.\n\n- Someone mixes up the test and live administration interfaces, and does some “funny” data entry. Customers spot a $1M “Bill Gates–shaped personal massager” on sale in your online store.\n\nTo appropriate the old saying that “a lie can travel halfway around the world while the truth is putting on its shoes,” in this day and age, a screw-up can be Dugg, Twittered, and Flibflarbed before anyone in the developer’s time zone is awake to do anything about it.\n\nEven your source code isn’t necessarily free of scrutiny. In 2004, when a tarball of the Windows 2000 source code made its way onto file-sharing networks, some folks merrily grepped through it for profanity, insults, and other funny content. (The comment `// TERRIBLE HORRIBLE NO GOOD VERY BAD HACK` has, I will admit, become appropriated by me from time to time since!) In summary, when writing any text in your code—whether comments, logging, dialogs, or test data—always ask yourself how it will look if it becomes public. It will save some red faces all around."
    },
    {
      "title": "Don’t Ignore That Error!",
      "author": "Pete Goodliffe",
      "content": "> I was walking down the street one evening to meet some friends in a bar. We hadn’t shared a beer in some time, and I was looking forward to seeing them again. In my haste, I wasn’t looking where I was going. I tripped over the edge of a curb and ended up flat on my face. Well, it serves me right for not paying attention, I guess.\nIt hurt my leg, but I was in a hurry to meet my friends. So, I pulled myself up and carried on. As I walked farther, the pain was getting worse. Although I’d initially dismissed it as shock, I rapidly realized there was something wrong.\nBut I hurried on to the bar regardless. I was in agony by the time I arrived. I didn’t have a great night out, because I was terribly distracted. In the morning, I went to the doctor and found out I’d fractured my shin bone. Had I stopped when I felt the pain, I would’ve prevented a lot of extra damage that I caused by walking on it. Probably the worst morning after of my life.\nToo many programmers write code like my disastrous night out.\n\n*Error, what error? It won’t be serious. Honestly. I can ignore it*. This is not a winning strategy for solid code. In fact, it’s just plain laziness. (The wrong sort.) No matter how unlikely you think an error is in your code, you should always check for it, and always handle it. Every time. You’re not saving time if you don’t; you’re storing up potential problems for the future.\n\nWe report errors in our code in a number of ways, including:\n\n- **Return codes** can be used as the resulting value of a function to mean “it didn’t work.” Error return codes are far too easy to ignore. You won’t see anything in the code to highlight the problem. Indeed, it’s become normal practice to ignore some standard C functions’ return values. How often do you check the return value from `printf`?\n\n- `errno` is a curious C aberration, a separate global variable set to signal error. It’s easy to ignore, hard to use, and leads to all sorts of nasty ­ problems—for example, what happens when you have multiple threads calling the same function? Some platforms insulate you from pain here; others do not.\n\n- **Exceptions** are a more structured language-supported way of signaling and handling errors. And you can’t possibly ignore them. Or can you? I’ve seen lots of code like this: \n```javascript\ntry { \n// ...do something...\n} \ncatch (...) {} // ignore errors \n```\nThe saving grace of this awful construct is that it highlights the fact that you’re doing something morally dubious.\n\nIf you ignore an error, turn a blind eye, and pretend that nothing has gone wrong, you run great risks. Just as my leg ended up in a worse state than if I’d stopped walking on it immediately, plowing on regardless of the red flags can lead to very complex failures. Deal with problems at the earliest opportunity. Keep a short account.\n\nNot handling errors leads to:\n\n- **Brittle code.** Code that’s filled with exciting, hard-to-find bugs.\n\n- **Insecure code.** Crackers often exploit poor error handling to break into software systems.\n\n- **Poor structure.** If there are errors from your code that are tedious to deal with continually, you probably have a poor interface. Express it so that the errors are less intrusive and their handling is less onerous.\n\nJust as you should check all potential errors in your code, you need to expose all potentially erroneous conditions in your interfaces. Do not hide them, pretending that your services will always work.\n\nWhy don’t we check for errors? There are a number of common excuses.\n\nWhich of these do you agree with? How would you counter each one?\n\n- Error handling clutters up the flow of the code, making it harder to read, and harder to spot the “normal” flow of execution.\n\n- It’s extra work, and I have a deadline looming.\n\n- I know that this function call will never return an error (`printf` always works, `malloc` always returns new memory—if it fails, we have bigger problems…).\n\n- It’s only a toy program, and needn’t be written to a production-worthy level."
    },
    {
      "title": "Don’t Just Learn the Language, Understand Its Culture",
      "author": "Anders Norås",
      "content": "In high school, I had to learn a foreign language. At the time, I thought that I’d get by nicely being good at English, so I chose to sleep through three years of French class. A few years later, I went to Tunisia on vacation.\n\nArabic is the official language there and, being a former French colony, French is also commonly used. English is only spoken in the touristy areas. Because of my linguistic ignorance, I found myself confined at the poolside reading *Finnegans Wake*, James Joyce’s tour de force in form and language. Joyce’s playful blend of more than 40 languages was a surprising, albeit exhausting, experience.\n\nRealizing how interwoven foreign words and phrases gave the author new ways of expressing himself is something I’ve kept with me in my programming career.\n\nIn their seminal book, *The Pragmatic Programmer* (Addison-Wesley Professional), Andy Hunt and Dave Thomas encourage us to learn a new programming language every year. I’ve tried to live by their advice, and throughout the years, I’ve had the experience of programming in many languages. My most important lesson from my polyglot adventures is that it takes more than just learning the syntax to learn a language: you need to understand its culture.\n\nYou can write Fortran in any language, but to truly learn a language you have to embrace it.\n\nDon’t make excuses if your C# code is a long `Main` method with mostly `static` helper methods, but learn why classes make sense. Don’t shy away if you have a hard time understanding the lambda expressions used in functional languages— force yourself to use them.\n\nOnce you’ve learned the ropes of a new language, you’ll be surprised how you’ll start using languages you already know in new ways.\n\nI learned how to use delegates effectively in C# from programming Ruby; releasing the full potential of .NET’s generics gave me ideas on how I could make Java generics more useful; and LINQ made it a breeze to teach myself Scala.\n\nYou’ll also get a better understanding of design patterns by moving between different languages. C programmers find that C# and Java have commoditized the iterator pattern. In Ruby and other dynamic languages, you might still use a visitor, but your implementation won’t look like the example from the Gang of Four book.\n\nSome might argue that *Finnegans Wake* is unreadable, while others applaud it for its stylistic beauty. To make the book a less daunting read, single language translations are available. Ironically, the first of these was in French.\n\nCode is in many ways similar. If you write *Wakese* code with a little Python, some Java, and a hint of Erlang, your projects will be a mess. If you instead explore new languages to expand your mind and get fresh ideas on how you can solve things in different ways, you will find that the code you write in your trusty old language gets more beautiful for every new language you’ve learned."
    },
    {
      "title": "Don’t Nail Your Program into the Upright Position",
      "author": "Verity Stob",
      "content": "I once wrote a spoof C++ quiz, in which I satirically suggested the following strategy for exception handling: \n\n> By dint of plentiful `try...catch` constructs throughout our codebase, we are sometimes able to prevent our applications from aborting. We think of the resultant state as “nailing the corpse in the upright position.”\n\nDespite my levity, I was actually summarizing a lesson I received at the knee of Dame Bitter Experience herself.\n\nIt was a base application class in our own, homemade C++ library. It had suffered the pokings of many programmers’ fingers over the years: nobody’s hands were clean. It contained code to deal with all escaped exceptions from everything else. Taking our lead from Yossarian in *Catch-22*, we decided, or rather felt (*decided* implies more thought than went into the construction of this monster) that an instance of this class should live forever or die in the attempt.\n\nTo this end, we intertwined multiple exception handlers. We mixed in Windows’ structured exception handling with the native kind (remember `__try...__except` in C++? Me, neither). When things threw unexpectedly, we tried calling them again, pressing the parameters harder. Looking back, I like to think that when writing an inner `try...catch` handler within the catch clause of another, some sort of awareness crept over me that I might have accidentally taken a slip road from the motorway of good practice into the aromatic but insalubrious lane of lunacy. However, this is probably retrospective wisdom.\n\nNeedless to say, whenever something went wrong in applications based on this class, they vanished like Mafia victims at the dockside, leaving behind no useful trail of bubbles to indicate what the hell happened, notwithstanding the dump routines that were supposedly called to record the disaster. Eventually—a long eventually—we took stock of what we had done, and experienced shame. We replaced the whole mess with a minimal and robust reporting mechanism. But this was many crashes down the line.\n\nI wouldn’t bother you with this—for surely nobody else could ever be as stupid as we were—but for an online argument I had recently with a bloke whose academic job title declared he should know better. We were discussing Java code in a remote transaction. If the code failed, he argued, it should catch and block the exception *in situ*. (“And then do *what* with it?” I asked. “Cook it for supper?”) He quoted the UI designers’ rule: NEVER LET THE USER SEE AN EXCEPTION REPORT, rather as though this settled the matter, what with it being in caps and everything. I wonder if he was responsible for the code in one of those blue-screened ATMs whose photos decorate the feebler blogs, and had been permanently traumatized.\n\nAnyway, if you should meet him, nod and smile and take no notice, as you sidle toward the door."
    },
    {
      "title": "Don’t Rely on “Magic Happens Here”",
      "author": "Alan Griffiths",
      "content": "if you look at any activity , process, or discipline from far enough away, it looks simple. Managers with no experience of development think what programmers do is simple, and programmers with no experience of management think the same of what managers do.\n\nProgramming is something some people do—some of the time. And the hard part—the thinking—is the least visible and least appreciated by the uninitiated.\n\nThere have been many attempts to remove the need for this skilled thinking over the decades. One of the earliest and most memorable is the effort by Grace Hopper to make programming languages less cryptic—which some accounts predicted would remove the need for specialist programmers. The result (COBOL) has contributed to the income of many specialist programmers over subsequent decades.\n\nThe persistent vision that software development can be simplified by removing programming is, to the programmer who understands what is involved, obviously naïve. But the mental process that leads to this mistake is part of human nature, and programmers are just as prone to making it as everyone else.\n\nOn any project, there are likely many things that an individual programmer doesn’t get actively involved in: eliciting requirements from users, getting budgets approved, setting up the build server, deploying the application to QA and production environments, migrating the business from the old processes or programs, etc.\n\nWhen you aren’t actively involved in things, there is an unconscious tendency to assume that they are simple and happen “by magic.” While the magic continues to happen, all is well. But when—it is usually “when” and not “if ”—the magic stops, the project is in trouble.\n\nI’ve seen projects lose weeks of developer time because no one understood how they relied on “the right” version of a DLL being loaded. When things started failing intermittently, team members looked everywhere else before someone noticed that “a wrong” version of the DLL was being loaded.\n\nAnother department was running smoothly—projects delivered on time, no late-night debugging sessions, no emergency fixes. So smoothly, in fact, that senior management decided that things “ran themselves,” and it could do without the project manager. Within six months, the projects in the department looked just like the rest of the organization—late, buggy, and continually being patched.\n\nYou don’t have to understand all the magic that makes your project work, but it doesn’t hurt to understand some of it—or to appreciate someone who understands the bits you don’t.\n\nMost importantly, make sure that when the magic stops, it can be started again."
    },
    {
      "title": "Don’t Repeat Yourself",
      "author": "Steve Smith",
      "content": "Of all the principles of programming, don’t repeat yourself (DRY) is perhaps one of the most fundamental. The principle was formulated by Andy Hunt and Dave Thomas in *The Pragmatic Programmer*, and underlies many other wellknown software development best practices and design patterns. The developer who learns to recognize duplication, and understands how to eliminate it through appropriate practice and proper abstraction, can produce much cleaner code than one who continuously infects the application with unnecessary repetition.\n\n**Duplication Is Waste**\nEvery line of code that goes into an application must be maintained, and is a potential source of future bugs. Duplication needlessly bloats the codebase, resulting in more opportunities for bugs and adding accidental complexity to the system. The bloat that duplication adds to the system also makes it more difficult for developers working with the system to fully understand the entire system, or to be certain that changes made in one location do not also need to be made in other places that duplicate the logic they are working on. DRY requires that “every piece of knowledge must have a single, unambiguous, authoritative representation within a system.” \n\n**Repetition in Process Calls for Automation**\nMany processes in software development are repetitive and easily automated. The DRY principle applies in these contexts, as well as in the source code of the application. Manual testing is slow, error-prone, and difficult to repeat, so automated test suites should be used where possible. Integrating software can be time consuming and error-prone if done manually, so a build process should be run as frequently as possible, ideally with every check-in. Wherever painful manual processes exist that can be automated, they should be automated and standardized. The goal is to ensure that there is only one way of accomplishing the task, and it is as painless as possible.\n\n**Repetition in Logic Calls for Abstraction**\nRepetition in logic can take many forms. Copy-and-paste *if-then* or *switch-case* logic is among the easiest to detect and correct. Many design patterns have the explicit goal of reducing or eliminating duplication in logic within an application. If an object typically requires several things to happen before it can be used, this can be accomplished with an Abstract Factory or a Factory Method pattern. If an object has many possible variations in its behavior, these behaviors can be injected using the Strategy pattern rather than large *if-then* structures. In fact, the formulation of design patterns themselves is an attempt to reduce the duplication of effort required to solve common problems and discuss such solutions. In addition, DRY can be applied to structures, such as database schema, resulting in normalization.\n\n**A Matter of Principle**\nOther software principles are also related to DRY. The Once and Only Once principle, which applies only to the functional behavior of code, can be thought of as a subset of DRY. The Open/Closed Principle, which states that “software entities should be open for extension, but closed for modification,” only works in practice when DRY is followed. Likewise, the well-known Single Responsibility Principle, which requires that a class have “only one reason to change,” relies on DRY.\n\nWhen followed with regard to structure, logic, process, and function, the DRY principle provides fundamental guidance to software developers and aids the creation of simpler, more maintainable, higher-quality applications. While there are scenarios where repetition can be necessary to meet performance or other requirements (e.g., data denormalization in a database), it should be used only where it directly addresses an actual rather than an imagined problem."
    },
    {
      "title": "Don’t Touch That Code!",
      "author": "Cal Evans",
      "content": "It has happened to every one of us at some point. Your code was rolled onto the staging server for system testing, and the testing manager writes back that she has hit a problem. Your first reaction is “Quick, let me fix that—I know what’s wrong.” In the bigger sense, though, what is wrong is that as a developer you think you should have access to the staging server.\n\nIn most web-based development environments, the architecture can be broken down like this:\n\n- Local development and unit testing on the developer’s machine \n\n- Development server where manual or automated integration testing is done\n\n- Staging server where the QA team and the users do acceptance testing \n\n- Production server \n\nYes, there are other servers and services sprinkled in there, like source code control and ticketing, but you get the idea. Using this model, a developer— even a senior developer—should never have access beyond the development server. Most development is done on a developer’s local machine using his favorite blend of IDEs, virtual machines, and an appropriate sprinkling of black magic for good luck.\n\nOnce checked into SCC, whether automatically or manually, it should be rolled over to the development server, where it can be tested and tweaked if necessary to make sure everything works together. From this point on, though, the developer is a spectator to the process.\n\nThe staging manager should package and roll the code to the staging server for the QA team. Just like developers should have no need to access anything beyond the development server, the QA team and the users have no need to touch anything on the development server. If it’s ready for acceptance testing, cut a release and roll; don’t ask the user to “just look at something real quick” on the development server. Remember, unless you are coding the project by yourself, other people have code there and they may not be ready for the user to see it. The release manager is the only person who should have access to both.\n\nUnder no circumstances—ever, at all—should a developer have access to a production server. If there is a problem, your support staff should either fix it or request that you fix it. After it’s checked into SCC, they will roll a patch from there. Some of the biggest programming disasters I’ve been a part of have taken place because someone \\**cough*\\* me \\**cough*\\* violated this last rule. If it’s broke, production is not the place to fix it."
    },
    {
      "title": "Encapsulate Behavior, Not Just State",
      "author": "Einar Landre",
      "content": "In systems theory, containment is one of the most useful constructs when dealing with large and complex system structures. In the software industry, the value of containment or encapsulation is well understood. Containment is supported by programming language constructs such as subroutines and functions, modules and packages, classes, and so on.\n\nModules and packages address the larger-scale needs for encapsulation, while classes, subroutines, and functions address the more fine-grained aspects of the matter. Over the years, I have discovered that classes seem to be one of the hardest encapsulation constructs for developers to get right. It’s not uncommon to find a class with a single 3,000-line `main` method, or a class with only `set` and `get` methods for its primitive attributes. These examples demonstrate that the developers involved have not fully understood object-oriented thinking, having failed to take advantage of the power of objects as modeling constructs. For developers familiar with the terms POJO (Plain Old Java Object) and POCO (Plain Old C# Object or Plain Old CLR Object), this was the intent in going back to the basics of OO as a modeling paradigm—the objects are plain and simple, but not dumb.\n\nAn object encapsulates both state and behavior, where the behavior is defined by the actual state. Consider a door object. It has four states: closed, open, closing, opening. It provides two operations: open and close. Depending on \n\nthe state, the open and close operations will behave differently. This inherent property of an object makes the design process conceptually simple. It boils down to two simple tasks: allocation and delegation of responsibility to the different objects including the interobject interaction protocols.\n\nHow this works in practice is best illustrated with an example. Let’s say we have three classes: `Customer`, `Order`, and `Item`. A `Customer` object is the natural placeholder for the credit limit and credit validation rules. An `Order` object knows about its associated `Customer`, and its `addItem` operation delegates the actual credit check by calling `customer.validateCredit(item.price())`. If the postcondition for the method fails, an exception can be thrown and the purchase aborted.\n\nLess experienced object-oriented developers might decide to wrap all the business rules into an object very often referred to as `OrderManager` or `OrderService`.\n\nIn these designs, `Order`, `Customer`, and `Item` are treated as little more than record types. All logic is factored out of the classes and tied together in one large, procedural method with a lot of internal *if-then-else* constructs. These methods are easily broken and are almost impossible to maintain. The reason? The encapsulation is broken.\n\nSo, in the end, don’t break the encapsulation, and use the power of your programming language to maintain it."
    },
    {
      "title": "Floating-Point Numbers Aren’t Real",
      "author": "Chuck Allison",
      "content": "Floating-point numbers are not “real numbers” in the mathematical sense, even though they are called real in some programming languages, such as pascal and fortran. Real numbers have infinite precision and are therefore continuous and nonlossy; floating-point numbers have limited precision, so they are finite, and they resemble “badly behaved” integers, because they’re not evenly spaced throughout their range.\n\nTo illustrate, assign 2147483647 (the largest signed 32-bit integer) to a 32-bit `float` variable (`x`, say), and print it. You’ll see 2147483648. Now print `x-64`. Still 2147483648. Now print `x-65`, and you’ll get 2147483520! Why? Because the spacing between adjacent floats in that range is 128, and floating-point operations round to the nearest floating-point number.\n\nIEEE floating-point numbers are fixed-precision numbers based on base-two scientific notation: 1.d,,1,,d,,2,,...d,,p,, 1 ×2^^e^^, where p is the precision (24 for `float`, 53 for `double`). The spacing between two consecutive numbers is 2^^1–p+e^^, which can be safely approximated by ε|x|, where ε is the *machine epsilon* (2^^1–p^^).\n\nKnowing the spacing in the neighborhood of a floating-point number can help you avoid classic numerical blunders. For example, if you’re performing an iterative calculation, such as searching for the root of an equation, there’s no sense in asking for greater precision than the number system can give in the neighborhood of the answer. Make sure that the tolerance you request is no smaller than the spacing there, otherwise you’ll loop forever.\n\nSince floating-point numbers are approximations of real numbers, there is inevitably a little error present. This error, called *roundoff*, can lead to surprising results.\n\nWhen you subtract nearly equal numbers, for example, the most significant digits cancel one another out, so what was the least significant digit (where the roundoff error resides) gets promoted to the most significant position in the floating-point result, essentially contaminating any further related computations (a phenomenon known as *smearing*). You need to look closely at your algorithms to prevent such *catastrophic cancellation*. To illustrate, consider solving the equation *x^^2^^ – 100000x + 1 = 0* with the quadratic formula. Since the operands in the expression *–b + sqrt(b^^2^^ – 4)* are nearly equal in magnitude, you can instead compute the root *r,,1,, = –b – sqrt(b^^2^^ – 4)*, and then obtain *r,,2,, = 1/r,,1,,*, since for any quadratic equation, *ax^^2^^ + bx + c = 0*, the roots satisfy r,,1,,r,,2,, = c/a.\n\nSmearing can occur in even more subtle ways. Suppose a library naïvely computes *e^^x^^* by the formula *1 + x + x^^2^^/2 + x^^3^^/3! + …*. This works fine for positive *x*, but consider what happens when x is a large negative number. The even-powered terms result in large positive numbers, and subtracting the odd-powered magnitudes will not even affect the result. The problem here is that the roundoff in the large, positive terms is in a digit position of much greater significance than the true answer. The answer diverges toward positive infinity! The solution here is also simple: for negative *x*, compute *e^^x^^ = 1/e^^|x|^^*.\n\nIt should go without saying that you shouldn’t use floating-point numbers for financial applications—that’s what decimal classes in languages like Python and C# are for. Floating-point numbers are intended for efficient scientific computation. But efficiency is worthless without accuracy, so remember the source of rounding errors, and code accordingly!"
    },
    {
      "title": "Fulfill Your Ambitions with Open Source",
      "author": "Richard Monson-Haefel",
      "content": "Chances are pretty good that you are not developing software at work that fulfills your most ambitious software development daydreams. Perhaps you are developing software for a huge insurance company when you would rather be working at Google, Apple, Microsoft, or your own startup developing the next big thing. You’ll never get where you want to go developing software for systems you don’t care about.\n\nFortunately, there is an answer to your problem: open source. There are thousands of open source projects out there, many of them quite active, which offer you any kind of software development experience you could want. If you love the idea of developing operating systems, go help with one of the dozen operating system projects. If you want to work on music software, animation software, cryptography, robotics, PC games, massive online player games, mobile phones, or whatever, you’ll almost certainly find at least one open source project dedicated to that interest.\n\nOf course, there is no free lunch. You have to be willing to give up your free time because you probably cannot work on an open source video game at your day job—you still have a responsibility to your employer. In addition, very few people make money contributing to open source projects—some do, but most don’t. You should be willing to give up some of your free time (less time playing video games and watching TV won’t kill you). The harder you work on an open source project, the faster you’ll realize your true ambitions as a programmer. It’s also important to consider your employee contract—some employers may restrict what you can contribute, even on your own time. In addition, you need to be careful about violating intellectual property laws having to do with copyright, patents, trademarks, and trade secrets.\n\nOpen source provides enormous opportunities for the motivated programmer. First, you get to see how someone else would implement a solution that interests you—you can learn a lot by reading other people’s source code. Second, you get to contribute your own code and ideas to the project—not every brilliant idea you have will be accepted, but some might, and you’ll learn something new just by working on solutions and contributing code. Third, you’ll meet great people with the same passion for the type of software that you have—these open source friendships can last a lifetime. Fourth, assuming you are a competent contributor, you’ll be able to add real-world experience in the technology that actually interests you.\n\nGetting started with open source is pretty easy. There is a wealth of documentation out there on the tools you’ll need (source code management, editors, programming languages, build systems, etc.). Find the project you want to work on first and learn about the tools that project uses. The documentation on projects themselves will be light in most cases, but this perhaps matters less because the best way to learn is to investigate the code yourself. If you want to get involved, you could offer to help out with the documentation. Or you could start by volunteering to write test code. While that may not sound exciting, the truth is you learn much faster by writing test code for other people’s software than almost any other activity in software. Write test code, really good test code. Find bugs, suggest fixes, make friends, work on software you like, and fulfill your software development ambitions."
    },
    {
      "title": "The Golden Rule of API Design",
      "author": "Michael Feathers",
      "content": "API design is tough, particularly in the large. If you are designing an API that is going to have hundreds or thousands of users, you have to think about how you might change it in the future and whether your changes might break client code. Beyond that, you have to think about how users of your API affect you. If one of your API classes uses one of its own methods internally, you have to remember that a user could subclass your class and override it, and that could be disastrous. You wouldn’t be able to change that method because some of your users have given it a different meaning. Your future internal implementation choices are at the mercy of your users.\n\nAPI developers solve this problem in various ways, but the easiest way is to lock down the API. If you are working in Java, you might be tempted to make most of your classes and methods `final`. In C#, you might make your classes and methods `sealed`. Regardless of the language you are using, you might be tempted to present your API through a singleton or use `static` factory methods to guard it from people who might override behavior and use your code in ways that may constrain your choices later. This all seems reasonable, but is it really?\n\nOver the past decade, we’ve gradually realized that unit testing is an extremely important part of practice, but that lesson has not completely permeated the industry. The evidence is all around us. Take an arbitrary untested class that uses a third-party API and try to write unit tests for it. Most of the time, you’ll run into trouble. You’ll find that the code using the API is stuck to it like glue. There’s no way to impersonate the API classes so that you can sense your code’s interactions with them, or supply return values for testing.\n\nOver time, this will get better, but only if we start to see testing as a real use case when we design APIs. Unfortunately, it’s a little bit more involved than just testing our code. That’s where the **Golden Rule of API Design** fits in: *It’s not enough to write tests for an API you develop; you have to write unit tests for code that uses your API*. When you follow this rule, you learn firsthand the hurdles that your users will have to overcome when they try to test their code independently.\n\nThere is no one way to make it easy for developers to test code that uses your API. `static`, `final`, and `sealed` are not inherently bad constructs. They can be useful at times. But it is important to be aware of the testing issue and, to do that, you have to experience it yourself. Once you have, you can approach it as you would any other design challenge."
    },
    {
      "title": "The Guru Myth",
      "author": "Ryan Brush",
      "content": "Anyone who has worked in software long enough has heard questions like this: \n\n> I’m getting exception xyz. Do you know what the problem is?\n\nThose asking the question rarely bother to include stack traces, error logs, or any context leading to the problem. They seem to think you operate on a different plane, that solutions appear to you without analysis based on evidence. They think you are a guru.\n\nWe expect such questions from those unfamiliar with software; to them, systems can seem almost magical. What worries me is seeing this in the software community. Similar questions arise in program design, such as “I’m building inventory management. Should I use optimistic locking?” Ironically, people asking the question are often better equipped to answer it than the question’s recipient. The questioners presumably know the context, know the requirements, and can read about the advantages and disadvantages of different strategies. Yet they expect you to give an intelligent answer without context. They expect magic.\n\nIt’s time for the software industry to dispel this guru myth. “Gurus” are human. They apply logic and systematically analyze problems like the rest of us. They tap into mental shortcuts and intuition. Consider the best programmer you’ve ever met: at one point, that person knew less about software than you do now. If someone seems like a guru, it’s because of years dedicated to learning and refining thought processes. A “guru” is simply a smart person with relentless curiosity.\n\nOf course, there remains a huge variance in natural aptitude. Many hackers out there are smarter, more knowledgeable, and more productive than I may ever be. Even so, debunking the guru myth has a positive impact. For instance, when working with someone smarter than me, I am sure to do the legwork, to provide enough context so that person can efficiently apply his or her skills. Removing the guru myth also means removing a perceived barrier to improvement. Instead of a magical barrier, I see a continuum along which I can advance.\n\nFinally, one of software’s biggest obstacles is smart people who purposefully propagate the guru myth. This might be done out of ego, or as a strategy to increase one’s value as perceived by a client or employer. Ironically, this attitude can make smart people less valuable, since they don’t contribute to the growth of their peers. We don’t need gurus. We need experts willing to develop other experts in their field. There is room for all of us."
    },
    {
      "title": "Hard Work Does Not Pay Off",
      "author": "Olve Maudal",
      "content": "As a programmer, you’ll find that working hard often does not pay off. You might fool yourself and a few colleagues into believing that you are contributing a lot to a project by spending long hours at the office. But the truth is that by working less, you might achieve more—sometimes much more. If you are trying to be focused and “productive” for more than 30 hours a week, you are probably working too hard. You should consider reducing your workload to become more effective and get more done.\n\nThis statement may seem counterintuitive and even controversial, but it is a direct consequence of the fact that programming and software development as a whole involve a continuous learning process. As you work on a project, you will understand more of the problem domain and, hopefully, find more effective ways of reaching the goal. To avoid wasted work, you must allow time to observe the effects of what you are doing, reflect on the things that you see, and change your behavior accordingly.\n\nProfessional programming is usually not like running hard for a few kilometers, where the goal can be seen at the end of a paved road. Most software projects are more like a long orienteering marathon. In the dark. With only a sketchy map as guidance. If you just set off in one direction, running as fast as you can, you might impress some, but you are not likely to succeed. You need to keep a sustainable pace, and you need to adjust the course when you learn more about where you are and where you are heading.\n\nIn addition, you always need to learn more about software development in general and programming techniques in particular. You probably need to read books, go to conferences, communicate with other professionals, experiment with new implementation techniques, and learn about powerful tools that simplify your job. As a professional programmer, you must keep yourself updated in your field of expertise—just as brain surgeons and pilots are expected to keep themselves up to date in their own fields of expertise. You need to spend evenings, weekends, and holidays educating yourself; therefore, you cannot spend your evenings, weekends, and holidays working overtime on your current project. Do you really expect brain surgeons to perform surgery 60 hours a week, or pilots to fly 60 hours a week? Of course not: preparation and education are an essential part of their profession.\n\nBe focused on the project, contribute as much as you can by finding smart solutions, improve your skills, reflect on what you are doing, and adapt your behavior. Avoid embarrassing yourself, and our profession, by behaving like a hamster in a cage spinning the wheel. As a professional programmer, you should know that trying to be focused and “productive” 60 hours a week is not a sensible thing to do. Act like a professional: prepare, effect, observe, reflect, and change."
    },
    {
      "title": "How to Use a Bug Tracker",
      "author": "Matt Doar",
      "content": "Whether you call them *bugs*, *defects*, or even *design side effects*, there is no getting away from them. Knowing how to submit a good bug report, as well as what to look for in one, are key skills for keeping a project moving along nicely.\n\nA good bug report needs to convey three things:\n\n- How to reproduce the bug, as precisely as possible, and how often this will make the bug appear\n\n- What should have happened, at least in your opinion\n\n- What actually happened, or at least as much information as you have recorded\n\nThe amount and quality of information reported in a bug says as much about the reporter as it does about the bug. Angry, terse bugs (“This function sucks!”) tell the developers that you were having a bad time, but not much else. A bug with plenty of context to make it easier to reproduce earns the respect of everyone, even if it stops a release.\n\nBugs are like a conversation, with all the history right there in front of everyone. Don’t blame others or deny the bug’s very existence. Instead, ask for more information or consider what you could have missed.\n\nChanging the status of a bug—e.g., *Open* to *Closed*—is a public statement of what you think of the bug. Taking the time to explain why you think the bug should be closed will save tedious hours spent later on justifying it to frustrated managers and customers. Changing the priority of a bug is a similar public statement, and just because it’s trivial to you doesn’t mean it isn’t stopping someone else from using the product.\n\nDon’t overload a bug’s fields for your own purposes. Adding “VITAL:” to the subject field may make it easier for you to sort the results of some report, but it will eventually be copied by others and inevitably mistyped, or will need to be removed for use in some other report. Use a new value or a new field instead, and document how the field is supposed to be used so other people don’t have to repeat themselves.\n\nMake sure that everyone knows how to find the bugs that the team is supposed to be working on. This can usually be done using a public query with an obvious name. Make sure everyone is using the same query, and don’t update this query without first informing the team that you’re changing what everyone is working on.\n\nFinally, remember that a bug is *not* a standard unit of work any more than a line of code is a precise measurement of effort."
    },
    {
      "title": "Improve Code by Removing It",
      "author": "Pete Goodliffe",
      "content": "Less is more. It’s a quite trite little maxim, but sometimes it really is true.\n\nOne of the improvements I’ve made to our codebase over the last few weeks is to remove chunks of it.\n\nWe’d written the software following XP tenets, including YAGNI (that is, You Aren’t Gonna Need It). Human nature being what it is, we inevitably fell short in a few places.\n\nI observed that the product was taking too long to execute certain tasks— simple tasks that should have been near instantaneous. This was because they were overimplemented—festooned with extra bells and whistles that were not required, but at the time had seemed like a good idea.\n\nSo I’ve simplified the code, improved the product performance, and reduced the level of global code entropy simply by removing the offending features from the codebase. Helpfully, my unit tests tell me that I haven’t broken anything else during the operation.\n\nA simple and thoroughly satisfying experience.\n\nSo why did the unnecessary code end up there in the first place? Why did one programmer feel the need to write extra code, and how did it get past review or the pairing process? Almost certainly something like:\n\n- It was a fun bit of extra stuff, and the programmer wanted to write it. *(Hint: Write code because it adds value, not because it amuses you.)* \n\n- Someone thought that it might be needed in the future, so felt it was best to code it now. *(Hint: That isn’t YAGNI. If you don’t need it right now, don’t write it right now.)* \n\n- It didn’t appear to be that big an “extra,” so it was easier to implement it rather than go back to the customer to see whether it was really required. *(Hint: It always takes longer to write and to maintain extra code. And the customer is actually quite approachable. A small, extra bit of code snowballs over time into a large piece of work that needs maintenance.)* \n\n- The programmer invented extra requirements that were neither documented nor discussed in order to justify the extra feature. The requirement was actually bogus. *(Hint: Programmers do not set system requirements; the customer does.)* \n\nWhat are you working on right now? Is it all needed?"
    },
    {
      "title": "Install Me",
      "author": "Marcus Baker",
      "content": "I am not the slightest bit interested in your program.\n\nI am surrounded by problems and have a to-do list as long as my arm. The only reason I am at your website right now is because I have heard an unlikely rumor that every one of my problems will be eliminated by your software.\n\nYou’ll forgive me if I’m skeptical.\n\nIf eyeball-tracking studies are correct, I’ve already read the title and I’m scanning for blue underlined text marked Download now. As an aside, if I arrived at this page with a Linux browser from a UK IP, chances are I would like the Linux version from a European mirror, so please don’t ask. Assuming the file dialog opens straight away, I consign the thing to my download folder and carry on reading.\n\nWe all constantly perform cost-benefit analysis of everything we do. If your project drops below my threshold for even a second, I will ditch it and go on to something else. Instant gratification is best.\n\nThe first hurdle is install. Don’t think that’s much of a problem? Go to your download folder now and have a look around. Full of .tar and .zip files, right?\n\nWhat percentage of those have you unpacked? How many have you installed?\n\nIf you are like me, only a third are doing more than acting as hard drive filler.\n\nI may want doorstep convenience, but I don’t want you entering my house uninvited. Before typing install, I would like to know exactly where you are putting stuff. It’s my computer, and I like to keep it tidy when I can. I also want to be able to remove your program the instant I am disenchanted with it. If I suspect that’s impossible, I won’t install it in the first place. My machine is stable right now, and I want to keep it that way.\n\nIf your program is GUI based, then I want to do something simple and see a result. Wizards don’t help, because they do stuff that I don’t understand.\n\nChances are, I want to read a file or write one. I don’t want to create projects, import directories, or tell you my email address. If all is working, on to the tutorial.\n\nIf your software is a library, then I carry on reading your web page looking for a quick start guide. I want the equivalent of “Hello world” in a five-line nobrainer with exactly the output described by your website. No big XML files or templates to fill out, just a single script. Remember, I have also downloaded your rival’s framework. You know, the one who always claims to be so much better than yours in the forums? If all is working, on to the tutorial.\n\nThere is a tutorial, isn’t there? One that talks to me in language I can understand?\n\nAnd if the tutorial mentions my problem, I’ll cheer up. Now that I’m reading about the things I can do, it starts to get interesting, fun even. I’ll lean back and sip my tea—did I mention I was from the UK?—and I’ll play with your examples and learn to use your creation. If it solves my problem, I’ll send you a thank-you email. I’ll send you bug reports when it crashes, and suggestions for features, too. I’ll even tell all my friends how your software is the best, even though I never did try your rival’s. And all because you took such care over my first tentative steps.\n\nHow could I ever have doubted you?"
    },
    {
      "title": "Interprocess Communication Affects Application Response Time",
      "author": "Randy Stafford",
      "content": "Response time is critical to software usability . Few things are as frustrating as waiting for some software system to respond, especially when our interaction with the software involves repeated cycles of stimulus and response. We feel as if the software is wasting our time and affecting our productivity. However, the causes of poor response time are less well appreciated, especially in modern applications. Much performance management literature still focuses on data structures and algorithms, issues that can make a difference in some cases but are far less likely to dominate performance in modern multitier enterprise applications.\n\nWhen performance is a problem in such applications, my experience has been that examining data structures and algorithms isn’t the right place to look for improvements. Response time depends most strongly on the number of remote interprocess communications (IPCs) conducted in response to a stimulus.\n\nWhile there can be other local bottlenecks, the number of remote interprocess communications usually dominates. Each remote interprocess communication contributes some nonnegligible latency to the overall response time, and these individual contributions add up, especially when they are incurred in sequence.\n\nA prime example is ripple loading in an application using object-relational mapping. *Ripple loading* describes the sequential execution of many database calls to select the data needed for building a graph of objects (see Lazy Load in Martin Fowler’s *Patterns of Enterprise Application Architecture* [AddisonWesley Professional]). When the database client is a middle-tier application server rendering a web page, these database calls are usually executed sequentially in a single thread. Their individual latencies accumulate, contributing to the overall response time. Even if each database call takes only 10 milliseconds, a page requiring 1,000 calls (which is not uncommon) will exhibit at least a 10-second response time. Other examples include web-service invocation, HTTP requests from a web browser, distributed object invocation, request– reply messaging, and data-grid interaction over custom network protocols.\n\nThe more remote IPCs needed to respond to a stimulus, the greater the response time will be.\n\nThere are a few relatively obvious and well-known strategies for reducing the number of remote interprocess communications per stimulus. One strategy is to apply the principle of parsimony, optimizing the interface between processes so that exactly the right data for the purpose at hand is exchanged with the minimum amount of interaction. Another strategy is to parallelize the interprocess communications where possible, so that the overall response time becomes driven mainly by the longest-latency IPC. A third strategy is to cache the results of previous IPCs, so that future IPCs may be avoided by hitting local cache instead.\n\nWhen you’re designing an application, be mindful of the number of interprocess communications in response to each stimulus. When analyzing applications that suffer from poor performance, I have often found IPC-to-stimulus ratios of thousands-to-one. Reducing this ratio, whether by caching or parallelizing or some other technique, will pay off much more than changing data structure choice or tweaking a sorting algorithm."
    },
    {
      "title": "Keep the Build Clean",
      "author": "Johannes Brodwall",
      "content": "Have you ever looked at a list of compiler warnings the length of an essay on bad coding and thought to yourself, “you know, i really should do something about that…but i don’t have time just now”? On the other hand, have you ever looked at a lone warning that appeared in a compilation and just fixed it?\n\nWhen I start a new project from scratch, there are no warnings, no clutter, no problems. But as the codebase grows, if I don’t pay attention, the clutter, the cruft, the warnings, and the problems can start piling up. When there’s a lot of noise, it’s much harder to find the warning that I really want to read among the hundreds of warnings I don’t care about.\n\nTo make warnings useful again, I try to use a zero-tolerance policy for warnings from the build. Even if the warning isn’t important, I deal with it. If it’s not critical but still relevant, I fix it. If the compiler warns about a potential null-pointer exception, I fix the cause—even if I “know” the problem will never show up in production. If the embedded documentation (Javadoc or similar) refers to parameters that have been removed or renamed, I clean up the documentation.\n\nIf it’s something I really don’t care about and that really doesn’t matter, I ask the team if we can change our warning policy. For example, I find that documenting the parameters and return value of a method in many cases doesn’t add any value, so it shouldn’t be a warning if they are missing. Or, upgrading to a new version of the programming language may make code that was previously OK now emit warnings. For example, when Java 5 introduced generics, all the old code that didn’t specify the generic type parameter would give a warning. This is a sort of warning I don’t want to be nagged about (at least, not yet). Having a set of warnings that are out of step with reality does not serve anyone.\n\nBy making sure that the build is always clean, I will not have to decide that a warning is irrelevant every time I encounter it. Ignoring things is mental work, and I need to get rid of all the unnecessary mental work I can. Having a clean build also makes it easier for someone else to take over my work. If I leave the warnings, someone else will have to wade through what is relevant and what is not. Or more likely, that person will just ignore all the warnings, including the significant ones.\n\nWarnings from your build are useful. You just need to get rid of the noise to start noticing them. Don’t wait for a big cleanup. When something appears that you don’t want to see, deal with it right away. You should fix the source of the warning, suppress the warning, or fix the warning policies of your tool. Keeping the build clean is not just about keeping it free of compilation errors or test failures: warnings are also an important and critical part of code hygiene."
    },
    {
      "title": "Know How to Use Command-Line Tools",
      "author": "Carroll Robinson",
      "content": "Today , many software development tools are packaged in the form of integrated development environments (ides). Microsoft’s Visual Studio and the open source Eclipse are two popular examples, though there are many others. There is a lot to like about IDEs. Not only are they easy to use, but they also relieve the programmer of thinking about a lot of little details involving the build process.\n\nEase of use, however, has its downside. Typically, when a tool is easy to use, it’s because the tool is making decisions for you and doing a lot of things automatically, behind the scenes. Thus, if an IDE is the only programming environment that you ever use, you may never fully understand what your tools are actually doing. You click a button, some magic occurs, and an executable file appears in the project folder.\n\nBy working with command-line build tools, you will learn a lot more about what the tools are doing when your project is being built. Writing your own *make* files will help you to understand all of the steps (compiling, assembling, linking, etc.) that go into building an executable file. Experimenting with the many command-line options for these tools is a valuable educational experience as well. To get started with using command-line build tools, you can use open source command-line tools such as GCC, or you can use the ones supplied with your proprietary IDE. After all, a well-designed IDE is just a graphical frontend to a set of command-line tools.\n\nIn addition to improving your understanding of the build process, there are some tasks that can be performed more easily or more efficiently with command-line tools than with an IDE. For example, the search and replace capabilities provided by the grep and sed utilities are often more powerful than those found in IDEs. Command-line tools inherently support scripting, which allows for the automation of tasks such as producing scheduled daily builds, creating multiple versions of a project, and running test suites. In an IDE, this kind of automation may be more difficult (if not impossible) to do, as build options are usually specified using GUI dialog boxes and the build process is invoked with a mouse click. If you never step outside of the IDE, you may not even realize that these kinds of automated tasks are possible.\n\nBut wait. Doesn’t the IDE exist to make development easier and to improve the programmer’s productivity? Well, yes. The suggestion presented here is not that you should stop using IDEs. The suggestion is that you should “look under the hood” and understand what your IDE is doing for you. The best way to do that is to learn to use command-line tools. Then, when you go back to using your IDE, you’ll have a much better understanding of what it is doing for you and how you can control the build process. On the other hand, once you master the use of command-line tools and experience the power and flexibility that they offer, you may find that you prefer the command line over the IDE."
    },
    {
      "title": "Know Well More Than Two Programming Languages",
      "author": "Russel Winder",
      "content": "The psychology of programming: people have known for a long time now that programming expertise is related directly to the number of different programming paradigms that a programmer is comfortable with—that is, not that he just knows about or knows a bit of, but that he can genuinely program with.\n\nEvery programmer starts with one programming language. That language has a dominating effect on the way that programmer thinks about software. No matter how many years of experience the programmer gets using that language, if she stays with that language, she will know only that language. A one-language programmer is constrained in her thinking by that language.\n\nA programmer who learns a second language will be challenged, especially if that language has a different computational model than the first. C, Pascal, Fortran— all have the same fundamental computational model. Switching from Fortran to C introduces a few, but not many, challenges. Moving from C or Fortran to C++ or Ada introduces fundamental challenges in the way programs behave.\n\nMoving from C++ to Haskell is a significant change and hence a significant challenge. Moving from C to Prolog is a very definite challenge.\n\nWe can enumerate a number of paradigms of computation: procedural, objectoriented, functional, logic, dataflow, etc. Moving among these paradigms creates the greatest challenges.\n\nWhy are these challenges good? That has to do with the way we think about the implementation of algorithms and the idioms and patterns of implementation that apply. In particular, cross-fertilization is at the core of expertise. Idioms for problem solutions that apply in one language may not be possible in another language. Trying to *port* the idioms from one language to another teaches us about both languages and about the problem being solved.\n\nCross-fertilization in the use of programming languages has huge effects. Perhaps the most obvious is the increased and increasing use of declarative modes of expression in systems implemented in imperative languages. Anyone versed in functional programming can easily apply a declarative approach even when using a language such as C. Using declarative approaches generally leads to shorter and more comprehensible programs. C++, for instance, certainly takes this on board with its wholehearted support for generic programming, which almost necessitates a declarative mode of expression.\n\nThe consequence of all this is that it behooves every programmer to be well skilled in programming in at least two different paradigms, and ideally at least the aforementioned five. Programmers should always be interested in learning new languages, preferably from an unfamiliar paradigm. Even if their day job always uses the same programming language, the increased sophistication of use of that language when a person can cross-fertilize from other paradigms should not be underestimated. Employers should take this into account and allow room in their training budget for employees to learn languages that are not currently being used, as a way of increasing the sophistication of the languages that are being used.\n\nAlthough it’s a start, a one-week training course is not sufficient to learn a new language: it generally takes a good few months of use, even if part-time, to gain a proper working knowledge of a language. It is the idioms of use, not just the syntax and computational model, that are the important factors."
    },
    {
      "title": "Know Your IDE",
      "author": "Heinz Kabutz",
      "content": "In the 1980s, our programming environments were typically nothing better than glorified text editors...if we were lucky. Syntax highlighting, which we take for granted nowadays, was a luxury that certainly was not available to everyone. Pretty printers to format our code nicely were usually external tools that had to be run to correct our spacing. Debuggers were also separate programs run to step through our code, but with a lot of cryptic keystrokes.\n\nDuring the 1990s, companies began to recognize the potential income that they could derive from equipping programmers with better and more useful tools. The Integrated Development Environment (IDE) combined the previous editing features with a compiler, debugger, pretty printer, and other tools. During that time, menus and the mouse also became popular, which meant that developers no longer needed to learn cryptic key combinations to use their editors. They could simply select their command from the menu.\n\nIn the 21st century, IDEs have become so commonplace that they are given away for free by companies wishing to gain market share in other areas. The modern IDE is equipped with an amazing array of features. My favorite is automated refactoring, particularly *Extract Method*, where I can select and convert a chunk of code into a method. The refactoring tool will pick up all the parameters that need to be passed into the method, which makes it extremely easy to modify code. My IDE will even detect other chunks of code that could also be replaced by this method and ask me whether I would like to replace them, too.\n\nAnother amazing feature of modern IDEs is the ability to enforce style rules within a company. For example, in Java, some programmers have started making all parameters `final` (which, in my opinion, is a waste of time).\n\nHowever, since they have such a style rule, all I would need to do to follow it is set it up in my IDE: I would get a warning for any non-`final` parameter. Style rules can also be used to find probable bugs, such as comparing autoboxed objects for reference equality, e.g., using == on primitive values that are autoboxed into reference objects.\n\nUnfortunately, modern IDEs do not require us to invest effort to learn how to use them. When I first programmed C on Unix, I had to spend quite a bit of time learning how the vi editor worked, due to its steep learning curve. This time spent up front paid off handsomely over the years. I am even typing the draft of this article with vi. Modern IDEs have a very gradual learning curve, which can have the effect that we never progress beyond the most basic usage of the tool.\n\nMy first step in learning an IDE is to memorize the keyboard shortcuts. Since my fingers are on the keyboard when I’m typing my code, pressing *Ctrl+Shift+I* to inline a variable prevents breaking the flow, whereas switching to navigate a menu with my mouse interrupts it. These interruptions lead to unnecessary context switches, making me much less productive if I try to do everything the lazy way. The same rule also applies to keyboard skills: learn to touch type; you won’t regret the time invested up front.\n\nLastly, as programmers we have time-proven Unix streaming tools that can help us manipulate our code. For example, if during a code review, I noticed that the programmers had named lots of classes the same, I could find these very easily using the tools `find`, `sed`, `sort`, `uniq`, and `grep`, like this: \n\n```\n\nfind . -name \"*.java\" | sed 's/.*\\///' | sort | uniq -c | grep -v \"^ *1 \" | sort -r\n\n``` \n\nWe expect a plumber coming to our house to be able to use his blowtorch. Let’s spend a bit of time to study how to become more effective with our IDE."
    },
    {
      "title": "Know Your Limits",
      "author": "Greg Colvin",
      "content": "> Man’s got to know his limitations.\n—Dirty Harry \n\nYour resources are limited. You only have so much time and money to do your work, including the time and money needed to keep your knowledge, skills, and tools up to date. You can only work so hard, so fast, so smart, and so long. Your tools are only so powerful. Your target machines are only so powerful. So you have to respect the limits of your resources.\n\nHow to respect those limits? Know yourself, know your people, know your budgets, and know your stuff. Especially, as a software engineer, know the space and time complexity of your data structures and algorithms, and the architecture and performance characteristics of your systems. Your job is to create an optimal marriage of software and systems.\n\nSpace and time complexity are given as the function *O(f(n))*, which for *n* equal the size of the input is the asymptotic space or time required as *n* grows to infinity. Important complexity classes for *f(n)* include *ln(n)*, *n*, *n ln(n)*, *n^^e^^*, and *e^^n^^*. As graphing these functions clearly shows, as n gets bigger, *O(ln(n))* is ever so much smaller than *O(n)* and *O(n ln(n))*, which are ever so much smaller than *O(n^^e^^)* and *O(e^^n^^)*. As Sean Parent puts it, for achievable *n*, all complexity classes amount to near-constant, near-linear, or near-infinite.\n\n[See the big O chart here](https://www.freecodecamp.org/news/big-o-cheat-sheet-time-complexity-chart/)\n\nComplexity analysis is measured in terms of an abstract machine, but software runs on real machines. Modern computer systems are organized as hierarchies of physical and virtual machines, including language runtimes, operating systems, CPUs, cache memory, random-access memory, disk drives, and networks. This table shows the limits on random access time and storage capacity for a typical networked server.\n\n| Storage | Access time | Capacity |\n| --- | --- | --- |\n| **Register** | <1ns | 64b|\n| **Cache line** |  | 64B|\n| **L1 cashe** | 1ns | 64KB |\n| **L2 cashe** | 4ns | 8MB |\n| **RAM** | 20ns | 32GB |\n| **Disk** | 10ms | 10TB |\n| **LAN** | 20ms | > 1PB |\n| **Internet** | 100ms | > 1ZB |\n\nNote that capacity and speed vary by several orders of magnitude. Caching and lookahead are used heavily at every level of our systems to hide this variation, but they only work when access is predictable. When cache misses are frequent, the system will be thrashing. For example, to randomly inspect every byte on a hard drive could take 32 years. Even to randomly inspect every byte in RAM could take 11 minutes. Random access is not predictable. What is?\n\nThat depends on the system, but reaccessing recently used items and accessing items sequentially are usually a win.\n\nAlgorithms and data structures vary in how effectively they use caches. For instance:\n\n- Linear search makes good use of lookahead, but requires *O(n)* comparisons.\n\n- Binary search of a sorted array requires only *O(log(n))* comparisons.\n\n- Search of a van Emde Boas tree is *O(log(n))* and cache-oblivious.\n\nHow to choose? In the last analysis, by measuring. The table below shows the time required to search arrays of 64-bit integers via these three methods. On my computer:\n\n- Linear search is competitive for small arrays, but loses exponentially for larger arrays.\n\n- van Emde Boas wins hands down, thanks to its predictable access pattern.\n\n**Array Length VS Search Time**\n| Array Length | Linear | Binary | van Emde Boas |\n| --- | --- | --- | --- |\n| **8** | 50ns | 90ns | 40ns |\n| **64** | 180ns | 150ns | 70ns |\n| **512** | 1,200ns | 230ns | 100ns |\n| **4,096** | 17,000ns | 320ns | 160ns |\n\n> You pays your money and you takes your choice.\n—Punch"
    },
    {
      "title": "Know Your Next Commit",
      "author": "Dan Bergh Johnsson",
      "content": "I tapped three programmers on their shoulders and asked what they were doing. “I am refactoring these methods,” the first answered. “I am adding some parameters to this web action,” the second answered. The third answered, “I am working on this user story.” \n\nIt might seem that the first two were engrossed in the details of their work, while only the third could see the bigger picture, and that he had the better focus. However, when I asked when and what they would commit, the picture changed dramatically. The first two were pretty clear about what files would be involved, and would be finished within an hour or so. The third programmer answered, “Oh, I guess I will be ready within a few days. I will probably add a few classes and might change those services in some way.” \n\nThe first two did not lack a vision of the overall goal. They had selected tasks they thought led in a productive direction, and could be finished within a couple of hours. Once they had finished those tasks, they would select a new feature or refactoring to work on. All the code written was thus done with a clear purpose and a limited, achievable goal in mind.\n\nThe third programmer had not been able to decompose the problem and was working on all aspects at once. He had no idea of what it would take, basically doing speculative programming, hoping to arrive at some point where he would be able to commit. Most probably, the code written at the start of this long session was poorly matched for the solution that came out in the end.\n\nWhat would the first two programmers do if their tasks took more than two hours? After realizing they had taken on too much, they would most likely throw away their changes, define smaller tasks, and start over. To keep working would have lacked focus and led to speculative code entering the repository. Instead, changes would be thrown away, but the insights kept.\n\nThe third programmer might keep on guessing and desperately try to patch together his changes into something that could be committed. After all, you cannot throw away code changes you have done—that would be wasted work, wouldn’t it? Unfortunately, not throwing the code away leads to slightly odd code that lacks a clear purpose entering the repository.\n\nAt some point, even the commit-focused programmers might fail to find something useful they thought could be finished in two hours. Then, they would go directly into speculative mode, playing around with the code and, of course, throwing away the changes whenever some insight led them back on track. Even these seemingly unstructured hacking sessions have purpose: to learn about the code to be able to define a task that would constitute a productive step.\n\nKnow your next commit. If you cannot finish, throw away your changes, then define a new task you believe in with the insights you have gained. Do speculative experimentation whenever needed, but do not let yourself slip into speculative mode without noticing. Do not commit guesswork into your repository."
    },
    {
      "title": "Large, Interconnected Data Belongs to a Database",
      "author": "Diomidis Spinellis",
      "content": "If your application is going to handle a large, persistent, interconnected set of data elements, don’t hesitate to store it in a relational database. In the past, RDBMSs used to be expensive, scarce, complex, and unwieldy beasts. This is no longer the case. Nowadays, RDBMS systems are easy to find—it is likely that the system you’re using already has one or two installed. Some very capable RDBMSs, like MySQL and PostgreSQL, are available as open source software, so cost of purchase is no longer an issue. Even better, so-called embedded database systems can be linked as libraries directly into your application, requiring almost no setup or management—two notable open source ones are SQLite and HSQLDB. These systems are extremely efficient.\n\nIf your application’s data is larger than the system’s RAM, an indexed RDBMS table will perform orders of magnitude faster than your library’s map collection type, which will thrash virtual memory pages. Modern database offerings can easily grow with your needs. With proper care, you can scale up an embedded database to a larger database system when required. Later on, you can switch from a free, open source offering to a better-supported or more powerful proprietary system.\n\nOnce you get the hang of SQL, writing database-centric applications is a joy.\n\nAfter you’ve stored your properly normalized data in the database, it’s easy to extract facts efficiently with a readable SQL query; there’s no need to write any complex code. Similarly, a single SQL command can perform complex data changes. For one-off modifications—say, a change in the way you organize your persistent data—you don’t even need to write code: just fire up the database’s direct SQL interface. This same interface also allows you to experiment with queries, sidestepping a regular programming language’s compile–edit cycle.\n\nAnother advantage of basing your code around an RDBMS involves the handling of relationships between your data elements. You can describe consistency constraints on your data in a declarative way, avoiding the risk of the dangling pointers you get if you forget to update your data in an edge case. For example, you can specify that if a user is deleted, then the messages sent by that user should be removed as well.\n\nYou can also create efficient links between the entities stored in the database any time you want, simply by creating an index. There is no need to perform expensive and extensive refactorings of class fields. In addition, coding around a database allows multiple applications to access your data in a safe way. This makes it easy to upgrade your application for concurrent use and also to code each part of your application using the most appropriate language and platform. For instance, you could write the XML backend of a web-based application in Java, some auditing scripts in Ruby, and a visualization interface in [Processing](https://processing.org/). Finally, keep in mind that the RDBMS will sweat hard to optimize your SQL commands, allowing you to concentrate on your application’s functionality rather than on algorithmic tuning. Advanced database systems will even take advantage of multicore processors behind your back. And, as technology improves, so will your application’s performance."
    },
    {
      "title": "Learn Foreign Languages",
      "author": "Klaus Marquardt",
      "content": "Programmers need to communicate. A lot.\n\nThere are periods in a programmer’s life when most communication seems to be with the computer—more precisely, with the programs running on that computer. This communication is about expressing ideas in a machine-readable way.\n\nThis remains an exhilarating prospect: programs are ideas turned into reality, with virtually no physical substance involved.\n\nProgrammers need to be fluent in the language of the machine, whether real or virtual, and in the abstractions that can be related to that language via development tools. It is important to learn many different abstractions, otherwise some ideas become incredibly hard to express. Good programmers need to be able to stand outside their daily routine, to be aware of other languages that are expressive for other purposes. The time always comes when this pays off.\n\nBeyond communication with machines, programmers need to communicate with their peers. Today’s large projects are more social endeavors than simply the applied art of programming. It is important to understand and express more than the machine-readable abstractions can. Most of the best programmers I know are also very fluent in their mother tongue, and typically in other languages as well. This is not just about communication with others: speaking a language well also leads to a clarity of thought that is indispensable when abstracting a problem. And this is what programming is also about.\n\nBeyond communication with machine, self, and peers, a project has many stakeholders, most with a different or no technical background. They live in testing, quality, and deployment; in marketing and sales; they are end users in some office (or store or home). You need to understand them and their concerns. This is almost impossible if you cannot speak their language—the language of their world, their domain. While you might think a conversation with them went well, they probably didn’t.\n\nIf you talk to accountants, you need a basic knowledge of cost-center accounting, of tied capital, capital employed, et al. If you talk to marketing or lawyers, some of their jargon and language (and thus, their minds) should be familiar to you. All these domain-specific languages need to be mastered by someone in the project—ideally, the programmers. Programmers are ultimately responsible for bringing the ideas to life via a computer.\n\nAnd, of course, life is more than software projects. As noted by Charlemagne, *to know another language is to have another soul*. For your contacts beyond the software industry, you will appreciate knowing foreign languages. To know when to listen rather than talk. To know that most language is without words.\n\n> Whereof one cannot speak, thereof one must be silent.\n—Ludwig Wittgenstein"
    },
    {
      "title": "Learn to Estimate",
      "author": "Giovanni Asproni",
      "content": "As a programmer, you need to be able to provide estimates to your managers, colleagues, and users for the tasks you need to perform, so that they will have a reasonably accurate idea of the time, costs, technology, and other resources needed to achieve their goals.\n\nTo be able to estimate well, it is obviously important to learn some estimation techniques. First of all, however, it is fundamental to learn what estimates are, and what they should be used for—as strange as it may seem, many developers and managers don’t really know this.\n\nThe following exchange between a project manager and a programmer is not atypical: \n\n*Project Manager:* Can you give me an estimate of the time necessary to develop feature xyz?\n\n*Programmer:* One month.\n\n*Project Manager:* That’s far too long! We’ve only got one week.\n\n*Programmer:* I need at least three.\n\n*Project Manager:* I can give you two at most.\n\n*Programmer:* Deal!\n\nThe programmer, at the end, comes up with an “estimate” that matches what is acceptable for the manager. But since it is seen to be the programmer’s estimate, the manager will hold the programmer accountable to it. To understand what is wrong with this conversation, we need three definitions—estimate, target, and commitment:\n\n- An *estimate* is an approximate calculation or judgment of the value, number, quantity, or extent of something. This definition implies that an estimate is a factual measure based on hard data and previous experience—hopes and wishes must be ignored when calculating it. The definition also implies that, being approximate, an estimate cannot be precise, e.g., a development task cannot be estimated to last 234.14 days.\n\n- A *target* is a statement of a desirable business objective, e.g., “The system must support at least 400 concurrent users.”\n\n- A *commitment* is a promise to deliver specified functionality at a certain level of quality by a certain date or event. One example could be “The search functionality will be available in the next release of the product.” \n\nEstimates, targets, and commitments are independent from one another, but targets and commitments should be based on sound estimates. As Steve McConnell notes, “The primary purpose of software estimation is not to predict a project’s outcome; it is to determine whether a project’s targets are realistic enough to allow the project to be controlled to meet them.” Thus, the purpose of estimation is to make proper project management and planning possible, allowing the project stakeholders to make commitments based on realistic targets.\n\nWhat the manager in the preceding conversation was really asking the programmer was to make a commitment based on an unstated target that the manager had in mind, *not* to provide an estimate. The next time you are asked to provide an estimate, make sure everybody involved knows what they are talking about, and your projects will have a better chance of succeeding. Now it’s time to learn some techniques..."
    },
    {
      "title": "Learn to Say, “Hello, World”",
      "author": "Thomas Guest",
      "content": "Paul lee, username `leep`, more commonly known as hoppy, had a reputation as the local expert on programming issues. I needed help. I walked across to Hoppy’s desk and asked whether he could take a look at some code for me.\n\n“Sure,” said Hoppy, “pull up a chair.” I took care not to topple the empty cola cans stacked in a pyramid behind him.\n\n“What code?”\n\n“In a function in a file,” I said.\n\n“So, let’s take a look at this function.” Hoppy moved aside a copy of K&R and slid his keyboard in front of me.\n\n“Where’s the IDE?” Apparently, Hoppy had no IDE running, just some editor that I couldn’t operate. He grabbed back the keyboard. A few keystrokes later, we had the file open—it was quite a big file—and were looking at the function—it was quite a big function. He paged down to the conditional block I wanted to ask about.\n\n“What would this clause actually do if x is negative?” I asked. “Surely it’s wrong.” \n\nI’d been trying all morning to find a way to force x to be negative, but the big function in the big file was part of a big project, and the cycle of recompiling and then rerunning my experiments was wearing me down. Couldn’t an expert like Hoppy just tell me the answer?\n\nHoppy admitted he wasn’t sure. To my surprise, he didn’t reach for K&R.\n\nInstead, he copied the code block into a new editor buffer, reindented it, wrapped it up in a function. A short while later, he had coded up a main function that looped forever, prompting the user for input values, passing them to the function, printing out the result. He saved the buffer as a new file, *tryit.c*.\n\nAll of this I could have done for myself, though perhaps not as quickly. But his next step was wonderfully simple and, at the time, quite foreign to my way of working: \n\n```\n\n$ cc tryit.c && ./a.out \n\n```\n\nLook! His actual program, conceived just a few minutes earlier, was now up and running. We tried a few values and confirmed my suspicions (so I’d been right about something!) and *then* he cross-checked the relevant section of K&R. I thanked Hoppy and left, again taking care not to disturb his cola can pyramid.\n\nBack at my own desk, I closed down my IDE. I’d become so used to working on a big project within a big product that I’d started to think that was what I should be doing. A general-purpose computer can do little tasks, too. I opened a text editor and began typing: \n\n```\n\n#include <stdio.h> \n\nint main() \n\n{ \n\n    printf(\"Hello, World\\n\"); \n\n    return 0; \n\n}\n\n```"
    },
    {
      "title": "Let Your Project Speak for Itself",
      "author": "Daniel Lindner",
      "content": "Your project probably has a version control system in place.\n\nPerhaps it is connected to a continuous integration server that verifies correctness by automated tests. That’s great.\n\nYou can include tools for static code analysis in your continuous integration server to gather code metrics. These metrics provide feedback about specific aspects of your code, as well as their evolution over time. When you install code metrics, there will always be a red line that you do not want to cross. Let’s assume you started with 20% test coverage and never want to fall below 15%. Continuous integration helps you keep track of all these numbers, but you still have to check regularly. Imagine you could delegate this task to the project itself and rely on it to report when things get worse.\n\nYou need to give your project a voice. This can be done by email or instant messaging, informing the developers about the latest decline or improvement in numbers. But it’s even more effective to embody the project in your office by using an extreme feedback device (XFD).\n\nThe idea of XFDs is to drive a physical device such as a lamp, a portable fountain, a toy robot, or even a USB rocket launcher, based on the results of the automatic analysis. Whenever your limits are broken, the device alters its state.\n\nIn case of a lamp, it will light up, bright and obvious. You can’t miss the message even if you’re hurrying out the door to get home.\n\nDepending on the type of extreme feedback device, you can hear the build break, see the red warning signals in your code, or even smell your code smells. The devices can be replicated at different locations if you work on a distributed team. You can place a traffic light in your project manager’s office, indicating overall project health state. Your project manager will appreciate it.\n\nLet your creativity guide you in choosing an appropriate device. If your culture is rather geeky, you might look for ways to equip your team mascot with radio-controlled toys. If you want a more professional look, invest in sleek designer lamps. Search the Internet for more inspiration. Anything with a power plug or a remote control has the potential to be used as an extreme feedback device.\n\nThe extreme feedback device acts as the voice box of your project. The project now resides physically with the developers, complaining to or praising them according to the rules the team has chosen. You can drive this personification further by applying speech-synthesis software and a pair of loudspeakers. Now your project really speaks for itself."
    }
  ]
}